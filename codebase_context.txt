================================================================================
CODEBASE CONTEXT FOR LLM
================================================================================
Generated: 2025-06-22 16:39:48
Root Path: /home/sav/projects/fastapi-data-extractor
Excluded: docker/, data/, .gitignore, test_safe_storage.py, README.md, .dockerignore, .env files, cache, logs
Included: source code, configuration files (pyproject.toml, etc.)
================================================================================

📁 REPOSITORY STRUCTURE
==================================================
Complete directory tree showing all relevant files and folders:

fastapi-data-extractor/
├── app
│   ├── api
│   │   ├── v1
│   │   │   ├── modules
│   │   │   │   ├── __init__.py
│   │   │   │   └── kitchen.py
│   │   │   ├── __init__.py
│   │   │   └── router.py
│   │   └── __init__.py
│   ├── core
│   │   ├── storage
│   │   │   ├── __init__.py
│   │   │   ├── base.py
│   │   │   ├── factory.py
│   │   │   ├── local_storage.py
│   │   │   └── s3_storage.py
│   │   ├── __init__.py
│   │   ├── content_scraper.py
│   │   ├── extraction_engine.py
│   │   └── file_manager.py
│   ├── models
│   │   ├── schemas
│   │   │   ├── __init__.py
│   │   │   └── recipes.py
│   │   ├── __init__.py
│   │   ├── base.py
│   │   ├── requests.py
│   │   └── responses.py
│   ├── services
│   │   ├── __init__.py
│   │   ├── extraction_service.py
│   │   └── schema_registry.py
│   ├── utils
│   │   ├── __init__.py
│   │   ├── exceptions.py
│   │   └── logging_manager.py
│   ├── __init__.py
│   ├── config.py
│   └── main.py
├── codebase_context.txt
└── pyproject.toml

==================================================

📄 SOURCE CODE AND CONFIGURATION FILES
==================================================

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: pyproject.toml
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
[project]
name = "fastapi-data-extractor"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "fastapi[standard]>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "openai>=1.54.0",
    "pydantic>=2.10.0",
    "pydantic-settings>=2.6.0",
    "playwright>=1.49.0",
    "html2text>=2024.2.26",
    "logfire>=0.54.0",
    "python-multipart>=0.0.12",
    "aiofiles>=24.1.0",
    "loguru>=0.7.2",
    "youtube-transcript-api>=1.1.0",
    "yt-dlp>=2025.6.9",
    "boto3>=1.35.0",
]


[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.24.0",
    "httpx>=0.28.0",
    "black>=24.0.0",
    "ruff>=0.8.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["app"]

[tool.uv]
dev-dependencies = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.24.0",
    "httpx>=0.28.0",
    "black>=24.0.0",
    "ruff>=0.8.0",
]


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/v1/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from .router import api_router

__all__ = ["api_router"]


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/v1/modules/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# Domain modules package


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/v1/modules/kitchen.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Dict, Any
from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form
from ....models.requests import ExtractionRequest, AudioUploadRequest
from ....models.responses import ExtractionResult
from ....services.extraction_service import ExtractionService
from ....utils.logging_manager import LoggingManager
import base64
import tempfile
import os

logger = LoggingManager.get_logger(__name__)


router = APIRouter()


def get_extraction_service() -> ExtractionService:
    """Dependency to get extraction service instance"""
    return ExtractionService()


@router.post("/extract-recipe", response_model=ExtractionResult)
async def extract_recipe_data(
    request: ExtractionRequest,
    service: ExtractionService = Depends(get_extraction_service),
) -> ExtractionResult:
    """
    Extract structured recipe data from multiple input types:
    - text: Direct text input
    - url: Web page URL
    - image: Image file (OCR)
    - audio: Audio file (transcription)
    - youtube_url: YouTube video (transcript)
    """
    try:
        request.extraction_type = "recipes"
        logger.info(f"Processing recipe extraction from {request.input_type.value}")
        return await service.process_extraction(request)
    except Exception as e:
        logger.error(f"Recipe extraction failed: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Recipe extraction failed: {str(e)}"
        )


@router.post("/extract-ingredients", response_model=ExtractionResult)
async def extract_ingredients_data(
    request: ExtractionRequest,
    service: ExtractionService = Depends(get_extraction_service),
) -> ExtractionResult:
    """Extract ingredients list from recipe content"""
    try:
        request.extraction_type = "ingredients"
        logger.info("Processing ingredients extraction request")
        return await service.process_extraction(request)
    except Exception as e:
        logger.error(f"Ingredients extraction failed: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Ingredients extraction failed: {str(e)}"
        )


@router.post("/extract-recipe-from-file", response_model=ExtractionResult)
async def extract_recipe_from_file(
    file: UploadFile = File(...),
    service: ExtractionService = Depends(get_extraction_service),
) -> ExtractionResult:
    """Upload an image file and extract recipe data"""
    try:
        # Read file content
        file_content = await file.read()
        base64_image = base64.b64encode(file_content).decode("utf-8")

        # Create request
        request = ExtractionRequest(
            input_type="image",
            content=f"data:image/{file.content_type.split('/')[-1]};base64,{base64_image}",
            extraction_type="recipes",
            save_json=True,
        )

        return await service.process_extraction(request)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/extract-recipe-from-audio", response_model=ExtractionResult)
async def extract_recipe_from_audio(
    audio_file: UploadFile = File(..., description="Audio file to transcribe"),
    save_markdown: bool = Form(
        default=False, description="Save transcription as markdown"
    ),
    save_json: bool = Form(default=True, description="Save extracted JSON data"),
    output_directory: str = Form(default=None, description="Custom output directory"),
    filename_prefix: str = Form(default=None, description="Custom filename prefix"),
    custom_instructions: str = Form(
        default=None, description="Additional extraction instructions"
    ),
    service: ExtractionService = Depends(get_extraction_service),
) -> ExtractionResult:
    """Upload an audio file, transcribe it using OpenAI Whisper, and extract recipe data"""
    temp_file_path = None
    try:
        # Validate audio file type
        if not audio_file.content_type.startswith("audio/"):
            raise HTTPException(
                status_code=400,
                detail=f"Invalid file type. Expected audio file, got {audio_file.content_type}",
            )

        # Save uploaded file to temporary location
        file_content = await audio_file.read()

        # Get file extension from content type or filename
        file_extension = ".wav"  # Default
        if audio_file.content_type == "audio/mpeg":
            file_extension = ".mp3"
        elif audio_file.content_type == "audio/wav":
            file_extension = ".wav"
        elif audio_file.content_type == "audio/m4a":
            file_extension = ".m4a"
        elif audio_file.content_type == "audio/ogg":
            file_extension = ".ogg"
        elif audio_file.filename:
            # Extract from filename if available
            original_ext = os.path.splitext(audio_file.filename)[1]
            if original_ext in [".mp3", ".wav", ".m4a", ".ogg", ".webm"]:
                file_extension = original_ext

        # Create temporary file
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=file_extension)
        temp_file.write(file_content)
        temp_file.close()
        temp_file_path = temp_file.name

        logger.info(
            f"Processing audio file: {audio_file.filename} ({audio_file.content_type})"
        )

        # Create extraction request
        request = ExtractionRequest(
            input_type="audio",
            content=temp_file_path,
            extraction_type="recipes",
            save_markdown=save_markdown,
            save_json=save_json,
            output_directory=output_directory,
            filename_prefix=filename_prefix,
            custom_instructions=custom_instructions,
        )

        # Process extraction
        result = await service.process_extraction(request)

        logger.success(f"Successfully processed audio file: {audio_file.filename}")
        return result

    except Exception as e:
        logger.error(f"Audio recipe extraction failed: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Audio recipe extraction failed: {str(e)}"
        )
    finally:
        # Clean up temporary file
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.unlink(temp_file_path)
            except Exception as e:
                logger.warning(
                    f"Failed to clean up temporary file {temp_file_path}: {str(e)}"
                )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/v1/router.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from fastapi import APIRouter
from .modules import kitchen

api_router = APIRouter()

# Only kitchen module with single tag
api_router.include_router(
    kitchen.router, prefix="/kitchen", tags=["kitchen"]  # ← Solo un tag = una sezione
)


@api_router.get("/modules")
async def get_available_modules():
    """Get information about available modules"""
    return {"available_modules": ["kitchen"], "total_modules": 1}


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/config.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import os
from pathlib import Path
from pydantic_settings import BaseSettings
from typing import Literal


class Settings(BaseSettings):
    # API Configuration
    app_name: str = "Structured Extraction API"
    version: str = "0.1.0"
    debug: bool = True

    # OpenAI Configuration
    openai_api_key: str
    openai_model: str = "gpt-4o-2024-08-06"

    # Storage Configuration
    storage_backend: Literal["local", "s3"] = "local"
    data_dir: str = "data"
    markdown_output_dir: str = "data/markdown"
    json_output_dir: str = "data/json"

    # S3 Configuration (only used if storage_backend is "s3")
    s3_bucket_name: str = ""
    s3_region: str = "eu-west-3"
    s3_access_key_id: str = ""
    s3_secret_access_key: str = ""
    s3_endpoint_url: str = ""  # For custom S3-compatible services
    s3_prefix: str = "fastapi-data-extractor/"  # Prefix for all files in S3

    # Scraper Configuration
    playwright_headless: bool = True
    playwright_timeout: int = 60000

    # Logging Configuration
    log_level: str = "INFO"
    log_to_file: bool = True
    log_file_path: str = "logs/app.log"
    log_rotation: str = "10 MB"
    log_retention: str = "1 month"
    log_format: str = (
        "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}"
    )

    # File Management
    enable_versioning: bool = True
    max_file_size_mb: int = 10

    class Config:
        env_file = ".env"
        case_sensitive = False


settings = Settings()


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/content_scraper.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# app/core/content_scraper.py
import asyncio
import hashlib
import os
from pathlib import Path
from typing import Optional, Tuple
import tempfile
import base64

import html2text
from playwright.async_api import async_playwright

from ..config import settings
from ..utils.logging_manager import LoggingManager
from ..core.file_manager import FileManager

from youtube_transcript_api import YouTubeTranscriptApi
import re
from urllib.parse import urlparse, parse_qs
from openai import OpenAI

logger = LoggingManager.get_logger(__name__)


class ContentScraper:
    """Enhanced content scraper supporting multiple input types"""

    def __init__(self):
        self.file_manager = FileManager()
        self.openai_client = OpenAI(api_key=settings.openai_api_key)

    async def fetch_content(
        self,
        content: str,
        input_type: str,
        save_markdown: bool = False,
        output_directory: Optional[str] = None,
    ) -> Tuple[str, Optional[str]]:
        """
        Extract text content from various input types

        Returns:
            Tuple of (extracted_text, markdown_path)
        """
        try:
            if input_type == "text":
                return self._process_text(content, save_markdown, output_directory)
            elif input_type == "url":
                return await self._process_url_async(
                    content, save_markdown, output_directory
                )
            elif input_type == "image":
                return self._process_image_with_vision(
                    content, save_markdown, output_directory
                )
            elif input_type == "youtube_url":
                return self._process_youtube(content, save_markdown, output_directory)
            elif input_type == "audio":
                return self._process_audio_with_whisper(
                    content, save_markdown, output_directory
                )
            else:
                raise ValueError(f"Unsupported input type: {input_type}")

        except Exception as e:
            logger.error(f"Content extraction failed for {input_type}: {str(e)}")
            raise

    def _process_text(
        self, text: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process plain text input"""
        markdown_path = None
        if save_markdown:
            markdown_path = self.file_manager.save_markdown(
                text, None, output_dir, "text_input"
            )
        return text, markdown_path

    async def _process_url_async(
        self, url: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process URL input using web scraping"""
        logger.info(f"Processing URL: {url}")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            try:
                # Navigate to URL
                await page.goto(url, wait_until="networkidle")

                # Get page content
                html_content = await page.content()

                # Convert HTML to markdown
                h = html2text.HTML2Text()
                h.ignore_links = False
                h.ignore_images = True
                markdown_content = h.handle(html_content)

                # Clean up content
                content = markdown_content.strip()

                if not content:
                    raise ValueError("No content extracted from URL")

                logger.info(f"URL scraping extracted {len(content)} characters")

                # Save markdown if requested
                markdown_path = None
                if save_markdown:
                    # Generate filename from URL
                    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
                    markdown_path = self.file_manager.save_markdown(
                        content, url, output_dir, f"url_{url_hash}"
                    )

                return content, markdown_path

            except Exception as e:
                logger.error(f"URL scraping failed: {str(e)}")
                raise
            finally:
                await browser.close()

    def _process_image_with_vision(
        self, image_input: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process image using OpenAI Vision API - no PIL needed!"""
        logger.info("Processing image with OpenAI Vision API")

        try:
            # Prepare image for OpenAI (no PIL processing)
            image_url = self._prepare_image_for_openai(image_input)

            # Direct API call - no local image processing
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Extract all text content from this image. If it contains a recipe, cooking instructions, or food-related content, provide a detailed transcription.",
                            },
                            {"type": "image_url", "image_url": {"url": image_url}},
                        ],
                    }
                ],
                max_tokens=1500,
            )

            extracted_text = response.choices[0].message.content

            if not extracted_text or not extracted_text.strip():
                raise ValueError("No content extracted from image")

            logger.info(f"Vision API extracted {len(extracted_text)} characters")

            # Save markdown if requested
            markdown_path = None
            if save_markdown:
                markdown_path = self.file_manager.save_markdown(
                    extracted_text,
                    f"image_vision_{hash(image_input)}",
                    output_dir,
                    "image_vision",
                )

            return extracted_text.strip(), markdown_path

        except Exception as e:
            logger.error(f"Vision API processing failed: {str(e)}")
            raise

    def _prepare_image_for_openai(self, image_input: str) -> str:
        """Prepare image for OpenAI Vision API - no PIL needed!"""

        # URL? Pass through
        if image_input.startswith("http"):
            return image_input

        # Already base64? Pass through
        if image_input.startswith("data:image"):
            return image_input

        # File path? Convert to base64 (no PIL needed)
        if image_input.startswith("/") or image_input.startswith("./"):
            with open(image_input, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode("utf-8")
                # Simple extension detection
                ext = image_input.lower().split(".")[-1]
                mime_type = (
                    f"image/{ext}"
                    if ext in ["jpg", "jpeg", "png", "gif", "webp"]
                    else "image/jpeg"
                )
                return f"data:{mime_type};base64,{base64_image}"

        raise ValueError("Invalid image input format")

    def _process_youtube(
        self, youtube_url: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process YouTube URL to extract transcript"""
        logger.info(f"Processing YouTube URL: {youtube_url}")

        try:
            # Extract video ID from URL
            video_id = self._extract_youtube_id(youtube_url)
            if not video_id:
                raise ValueError("Invalid YouTube URL")

            # Get transcript
            transcript_list = YouTubeTranscriptApi.get_transcript(
                video_id,
                languages=[
                    "en",
                    "it",
                    "auto",
                ],  # Try English, Italian, then auto-generated
            )

            # Combine transcript text
            transcript_text = " ".join([entry["text"] for entry in transcript_list])

            if not transcript_text.strip():
                raise ValueError("No transcript available for this video")

            logger.info(
                f"YouTube transcript extracted: {len(transcript_text)} characters"
            )

            # Save markdown if requested
            markdown_path = None
            if save_markdown:
                markdown_path = self.file_manager.save_markdown(
                    transcript_text, youtube_url, output_dir, f"youtube_{video_id}"
                )

            return transcript_text.strip(), markdown_path

        except Exception as e:
            logger.error(f"YouTube transcript extraction failed: {str(e)}")
            raise

    def _extract_youtube_id(self, url: str) -> Optional[str]:
        """Extract video ID from YouTube URL"""
        patterns = [
            r"(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)",
            r"youtube\.com\/watch\?.*v=([^&\n?#]+)",
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        return None

    def _process_audio_with_whisper(
        self, audio_input: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process audio file using OpenAI Whisper API"""
        logger.info("Processing audio with OpenAI Whisper API")

        try:
            # Prepare audio file for OpenAI
            audio_file_path = self._prepare_audio_for_openai(audio_input)

            # Transcribe audio using Whisper
            with open(audio_file_path, "rb") as audio_file:
                transcript = self.openai_client.audio.transcriptions.create(
                    model="whisper-1", file=audio_file, response_format="text"
                )

            if not transcript or not transcript.strip():
                raise ValueError("No content extracted from audio")

            logger.info(f"Whisper API transcribed {len(transcript)} characters")

            # Save markdown if requested
            markdown_path = None
            if save_markdown:
                audio_hash = hashlib.md5(audio_input.encode()).hexdigest()[:8]
                markdown_path = self.file_manager.save_markdown(
                    transcript,
                    f"audio_transcription_{audio_hash}",
                    output_dir,
                    "audio_transcription",
                )

            # Clean up temporary file if created
            if audio_file_path != audio_input and os.path.exists(audio_file_path):
                os.unlink(audio_file_path)

            return transcript.strip(), markdown_path

        except Exception as e:
            logger.error(f"Whisper API processing failed: {str(e)}")
            raise

    def _prepare_audio_for_openai(self, audio_input: str) -> str:
        """Prepare audio for OpenAI Whisper API"""

        # If it's a file path, return it directly
        if os.path.isfile(audio_input):
            return audio_input

        # If it's base64 encoded audio, decode and save to temp file
        if audio_input.startswith("data:audio"):
            # Extract base64 data
            header, data = audio_input.split(",", 1)
            audio_data = base64.b64decode(data)

            # Extract file extension from header
            if "audio/wav" in header:
                ext = ".wav"
            elif "audio/mp3" in header:
                ext = ".mp3"
            elif "audio/m4a" in header:
                ext = ".m4a"
            elif "audio/ogg" in header:
                ext = ".ogg"
            else:
                ext = ".wav"  # Default

            # Create temporary file
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=ext)
            temp_file.write(audio_data)
            temp_file.close()

            return temp_file.name

        # If it's raw base64 without header, assume it's audio
        try:
            audio_data = base64.b64decode(audio_input)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            temp_file.write(audio_data)
            temp_file.close()
            return temp_file.name
        except Exception:
            pass

        raise ValueError(
            "Invalid audio input format. Provide file path or base64 encoded audio."
        )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/extraction_engine.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# app/core/extraction_engine.py
import time
from typing import Optional, Dict, Any, Tuple
from openai import OpenAI

from ..config import settings
from ..services.schema_registry import SchemaRegistry
from ..utils.logging_manager import LoggingManager
from ..core.content_scraper import ContentScraper

logger = LoggingManager.get_logger(__name__)


class ExtractionEngine:
    """Core engine for structured data extraction using OpenAI"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        self.api_key = api_key or settings.openai_api_key
        if not self.api_key:
            raise ValueError(
                "OpenAI API key is required. Provide it as parameter or set "
                "OPENAI_API_KEY environment variable."
            )

        self.client = OpenAI(api_key=self.api_key)
        self.model = model or settings.openai_model
        self.schema_registry = SchemaRegistry()

    def extract_structured_data(
        self,
        content: str,
        extraction_type: str,
        custom_instructions: Optional[str] = None,
    ) -> Tuple[Dict[str, Any], Dict[str, int], float]:
        """
        Extract structured data from content using specified schema

        Returns:
            Tuple of (extracted_data, token_usage, processing_time)
        """
        start_time = time.time()

        # Try registry first, fallback to direct import
        try:
            schema_class = self.schema_registry.get_schema(extraction_type)
        except ValueError:
            # Direct import fallback for new schemas
            if extraction_type == "recipes":
                from ..models.schemas.recipes import RecipeExtraction

                schema_class = RecipeExtraction
            elif extraction_type == "quotes":
                from ..models.schemas.quotes import QuotesExtraction

                schema_class = QuotesExtraction
            elif extraction_type == "insights":
                from ..models.schemas.insights import InsightsExtraction

                schema_class = InsightsExtraction
            else:
                raise ValueError(f"Unknown extraction type: {extraction_type}")

        # Build system prompt
        system_prompt = schema_class.prompt
        if custom_instructions:
            system_prompt += f"\n\nAdditional instructions: {custom_instructions}"

        logger.info(f"Starting extraction", extraction_type=extraction_type)

        try:
            completion = self.client.beta.chat.completions.parse(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user",
                        "content": (
                            f"Extract structured information from this content:\n\n{content}"
                        ),
                    },
                ],
                response_format=schema_class,
            )

            result = completion.choices[0].message.parsed
            token_usage = {
                "prompt_tokens": completion.usage.prompt_tokens,
                "completion_tokens": completion.usage.completion_tokens,
                "total_tokens": completion.usage.total_tokens,
            }

            processing_time = time.time() - start_time

            logger.success(
                f"Successfully extracted data",
                extraction_type=extraction_type,
                processing_time=round(processing_time, 2),
                tokens_used=token_usage["total_tokens"],
            )

            return result.model_dump(), token_usage, processing_time

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(
                f"Error extracting structured data",
                extraction_type=extraction_type,
                error=str(e),
                processing_time=round(processing_time, 2),
            )

            raise

    def extract_from_image_directly(
        self,
        image_input: str,
        extraction_type: str,
        custom_instructions: Optional[str] = None,
    ) -> Tuple[Dict[str, Any], Dict[str, int], float]:
        """Extract structured data directly from image using Vision API"""
        start_time = time.time()

        # Get schema class
        if extraction_type == "recipes":
            from ..models.schemas.recipes import RecipeExtraction

            schema_class = RecipeExtraction
        else:
            raise ValueError(
                f"Direct image extraction not supported for: {extraction_type}"
            )

        # Prepare image
        scraper = ContentScraper()
        image_url = scraper._prepare_image_for_openai(image_input)

        # Build prompt for direct extraction
        system_prompt = f"""You are extracting structured recipe data directly from an image. 
        {schema_class.prompt}
        
        Look at the image carefully and extract all recipe information you can see.
        If ingredients are listed, extract them with quantities.
        If cooking steps are shown, extract them in order.
        """

        if custom_instructions:
            system_prompt += f"\n\nAdditional instructions: {custom_instructions}"

        try:
            completion = self.client.beta.chat.completions.parse(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Extract recipe information from this image:",
                            },
                            {"type": "image_url", "image_url": {"url": image_url}},
                        ],
                    },
                ],
                response_format=schema_class,
            )

            result = completion.choices[0].message.parsed
            token_usage = {
                "prompt_tokens": completion.usage.prompt_tokens,
                "completion_tokens": completion.usage.completion_tokens,
                "total_tokens": completion.usage.total_tokens,
            }

            processing_time = time.time() - start_time

            logger.success(
                f"Direct image extraction completed",
                processing_time=round(processing_time, 2),
            )

            return result.model_dump(), token_usage, processing_time

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Direct image extraction failed: {str(e)}")
            raise


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/file_manager.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import hashlib
from datetime import datetime, timezone
from typing import Dict, Any, Optional

from ..config import settings
from ..utils.logging_manager import LoggingManager
from .storage.factory import StorageFactory
from .storage.base import StorageBackend

logger = LoggingManager.get_logger(__name__)


class FileManager:
    """Manages file operations for extracted data with configurable storage backends"""

    def __init__(self, storage_backend: Optional[StorageBackend] = None):
        self.storage = storage_backend or StorageFactory.create_storage()

    def _generate_filename(
        self,
        content_hash: str,
        extraction_type: str,
        prefix: Optional[str] = None,
        extension: str = ".json",
    ) -> str:
        """Generate filename with optional versioning"""
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")

        parts = []
        if prefix:
            parts.append(prefix)
        parts.extend([extraction_type, content_hash[:8], timestamp])

        return "_".join(parts) + extension

    def _get_content_hash(self, content: str) -> str:
        """Generate hash for content"""
        return hashlib.sha256(content.encode()).hexdigest()

    def save_json(
        self,
        data: Dict[str, Any],
        content: str,
        extraction_type: str,
        output_dir: Optional[str] = None,
        filename_prefix: Optional[str] = None,
    ) -> str:
        """Save extracted data as JSON file"""
        content_hash = self._get_content_hash(content)
        filename = self._generate_filename(
            content_hash, extraction_type, filename_prefix
        )

        # Determine the relative path for storage
        if output_dir:
            # Extract just the directory name if full path provided
            dir_name = output_dir.split("/")[-1] if "/" in output_dir else output_dir
            filepath = f"{dir_name}/{filename}"
        else:
            filepath = f"json/{filename}"

        # Prepare data with metadata
        save_data = {
            "metadata": {
                "extraction_type": extraction_type,
                "content_hash": content_hash,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "filename": filename,
                "storage_backend": settings.storage_backend,
            },
            "extracted_data": data,
        }

        # Save using the storage backend
        saved_path = self.storage.save_json(save_data, filepath)

        logger.info(
            f"Saved JSON file: {filename}",
            filepath=saved_path,
            extraction_type=extraction_type,
            storage_backend=settings.storage_backend,
        )

        return saved_path

    def save_markdown(
        self,
        content: str,
        source_url: Optional[str] = None,
        output_dir: Optional[str] = None,
        filename_prefix: Optional[str] = None,
    ) -> str:
        """Save markdown content to file"""
        content_hash = self._get_content_hash(content)
        filename = self._generate_filename(
            content_hash, "markdown", filename_prefix, ".md"
        )

        # Determine the relative path for storage
        if output_dir:
            # Extract just the directory name if full path provided
            dir_name = output_dir.split("/")[-1] if "/" in output_dir else output_dir
            filepath = f"{dir_name}/{filename}"
        else:
            filepath = f"markdown/{filename}"

        # Prepare markdown content with metadata
        markdown_content = ""
        if source_url:
            markdown_content += f"<!-- Source URL: {source_url} -->\n"
        markdown_content += f"<!-- Content Hash: {content_hash} -->\n"
        markdown_content += (
            f"<!-- Saved: {datetime.now(timezone.utc).isoformat()} -->\n"
        )
        markdown_content += f"<!-- Storage Backend: {settings.storage_backend} -->\n\n"
        markdown_content += content

        # Save using the storage backend
        saved_path = self.storage.save_file(
            markdown_content,
            filepath,
            metadata={
                "source_url": source_url or "",
                "content_hash": content_hash,
                "storage_backend": settings.storage_backend,
            },
        )

        logger.info(
            f"Saved markdown file: {filename}",
            filepath=saved_path,
            storage_backend=settings.storage_backend,
        )

        return saved_path


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# Storage module initialization
from .base import StorageBackend
from .local_storage import LocalStorage
from .s3_storage import S3Storage
from .factory import StorageFactory

__all__ = ["StorageBackend", "LocalStorage", "S3Storage", "StorageFactory"]


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/base.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional


class StorageBackend(ABC):
    """Abstract base class for storage backends"""

    @abstractmethod
    def save_file(
        self, content: str, filepath: str, metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Save content to a file

        Args:
            content: Content to save
            filepath: Path where to save the file
            metadata: Optional metadata to include

        Returns:
            Full path or URL of the saved file
        """
        pass

    @abstractmethod
    def save_json(
        self,
        data: Dict[str, Any],
        filepath: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Save JSON data to a file

        Args:
            data: JSON data to save
            filepath: Path where to save the file
            metadata: Optional metadata to include

        Returns:
            Full path or URL of the saved file
        """
        pass

    @abstractmethod
    def file_exists(self, filepath: str) -> bool:
        """Check if a file exists"""
        pass

    @abstractmethod
    def get_file_url(self, filepath: str) -> str:
        """Get the URL/path to access the file"""
        pass


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/factory.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Optional

from .base import StorageBackend
from .local_storage import LocalStorage
from .s3_storage import S3Storage
from ...config import settings
from ...utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)


class StorageFactory:
    """Factory class to create storage backends"""

    @staticmethod
    def create_storage(backend_type: Optional[str] = None) -> StorageBackend:
        """
        Create a storage backend based on configuration

        Args:
            backend_type: Override the configured backend type

        Returns:
            StorageBackend instance
        """
        backend = backend_type or settings.storage_backend

        if backend == "local":
            logger.info("Using local storage backend")
            return LocalStorage(base_dir=settings.data_dir)

        elif backend == "s3":
            logger.info("Using S3 storage backend")

            # Validate S3 configuration
            if not settings.s3_bucket_name:
                raise ValueError("S3 bucket name is required when using S3 storage")

            return S3Storage(
                bucket_name=settings.s3_bucket_name,
                region=settings.s3_region,
                access_key_id=settings.s3_access_key_id or None,
                secret_access_key=settings.s3_secret_access_key or None,
                endpoint_url=settings.s3_endpoint_url or None,
                prefix=settings.s3_prefix,
            )

        else:
            raise ValueError(f"Unsupported storage backend: {backend}")


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/local_storage.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import json
import os
from pathlib import Path
from typing import Dict, Any, Optional

from .base import StorageBackend
from ...utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)


class LocalStorage(StorageBackend):
    """Local filesystem storage backend"""

    def __init__(self, base_dir: str = "data"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)

    def save_file(
        self, content: str, filepath: str, metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Save content to local file"""
        full_path = self.base_dir / filepath
        full_path.parent.mkdir(parents=True, exist_ok=True)

        with open(full_path, "w", encoding="utf-8") as f:
            f.write(content)

        file_size = os.path.getsize(full_path)
        logger.info(
            f"Saved file locally: {filepath}",
            filepath=str(full_path),
            size_bytes=file_size,
        )

        return str(full_path)

    def save_json(
        self,
        data: Dict[str, Any],
        filepath: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Save JSON data to local file"""
        full_path = self.base_dir / filepath
        full_path.parent.mkdir(parents=True, exist_ok=True)

        save_data = data.copy()
        if metadata:
            save_data["metadata"] = {**save_data.get("metadata", {}), **metadata}

        with open(full_path, "w", encoding="utf-8") as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)

        file_size = os.path.getsize(full_path)
        logger.info(
            f"Saved JSON file locally: {filepath}",
            filepath=str(full_path),
            size_bytes=file_size,
        )

        return str(full_path)

    def file_exists(self, filepath: str) -> bool:
        """Check if file exists locally"""
        return (self.base_dir / filepath).exists()

    def get_file_url(self, filepath: str) -> str:
        """Get local file path"""
        return str(self.base_dir / filepath)


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/s3_storage.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import json
from typing import Dict, Any, Optional
from datetime import datetime, timezone

try:
    import boto3
    from botocore.exceptions import ClientError, NoCredentialsError

    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False

from .base import StorageBackend
from ...utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)


class S3Storage(StorageBackend):
    """S3 storage backend"""

    def __init__(
        self,
        bucket_name: str,
        region: str = "us-east-1",
        access_key_id: Optional[str] = None,
        secret_access_key: Optional[str] = None,
        endpoint_url: Optional[str] = None,
        prefix: str = "",
    ):
        if not BOTO3_AVAILABLE:
            raise ImportError(
                "boto3 is required for S3 storage. Install it with: pip install boto3"
            )

        self.bucket_name = bucket_name
        self.prefix = prefix.rstrip("/") + "/" if prefix else ""

        # Initialize S3 client
        # Se access_key_id e secret_access_key sono None o vuoti, boto3 userà le credenziali di default
        if access_key_id and secret_access_key:
            # Usa credenziali esplicite
            session = boto3.Session(
                aws_access_key_id=access_key_id,
                aws_secret_access_key=secret_access_key,
                region_name=region,
            )
            logger.info("Using explicit AWS credentials from environment")
        else:
            # Usa credenziali di default (aws configure, IAM role, etc.)
            session = boto3.Session(region_name=region)
            logger.info("Using default AWS credentials (aws configure or IAM role)")

        client_kwargs = {}
        if endpoint_url:
            client_kwargs["endpoint_url"] = endpoint_url

        self.s3_client = session.client("s3", **client_kwargs)

        # Verify bucket access
        self._verify_bucket_access()

    def _verify_bucket_access(self):
        """Verify that we can access the S3 bucket"""
        try:
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            logger.info(f"Successfully connected to S3 bucket: {self.bucket_name}")
        except ClientError as e:
            error_code = e.response["Error"]["Code"]
            if error_code == "404":
                raise ValueError(f"S3 bucket '{self.bucket_name}' does not exist")
            elif error_code == "403":
                raise ValueError(f"Access denied to S3 bucket '{self.bucket_name}'")
            else:
                raise ValueError(f"Error accessing S3 bucket '{self.bucket_name}': {e}")
        except NoCredentialsError:
            raise ValueError(
                "AWS credentials not found. Please run 'aws configure' or set credentials in environment variables."
            )

    def _get_s3_key(self, filepath: str) -> str:
        """Get the full S3 key including prefix"""
        return f"{self.prefix}{filepath.lstrip('/')}"

    def save_file(
        self, content: str, filepath: str, metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Save content to S3"""
        s3_key = self._get_s3_key(filepath)

        # Prepare metadata for S3
        s3_metadata = {
            "saved-at": datetime.now(timezone.utc).isoformat(),
            "content-type": "text/plain",
        }
        if metadata:
            # Convert metadata values to strings (S3 requirement)
            s3_metadata.update({k: str(v) for k, v in metadata.items()})

        try:
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=s3_key,
                Body=content.encode("utf-8"),
                ContentType="text/plain; charset=utf-8",
                Metadata=s3_metadata,
            )

            logger.info(
                f"Saved file to S3: {filepath}",
                bucket=self.bucket_name,
                s3_key=s3_key,
                size_bytes=len(content.encode("utf-8")),
            )

            return f"s3://{self.bucket_name}/{s3_key}"

        except ClientError as e:
            logger.error(f"Failed to save file to S3: {e}")
            raise

    def save_json(
        self,
        data: Dict[str, Any],
        filepath: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Save JSON data to S3"""
        s3_key = self._get_s3_key(filepath)

        save_data = data.copy()
        if metadata:
            save_data["metadata"] = {**save_data.get("metadata", {}), **metadata}

        json_content = json.dumps(save_data, indent=2, ensure_ascii=False)

        # Prepare metadata for S3
        s3_metadata = {
            "saved-at": datetime.now(timezone.utc).isoformat(),
            "content-type": "application/json",
        }
        if metadata:
            s3_metadata.update({k: str(v) for k, v in metadata.items()})

        try:
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=s3_key,
                Body=json_content.encode("utf-8"),
                ContentType="application/json; charset=utf-8",
                Metadata=s3_metadata,
            )

            logger.info(
                f"Saved JSON file to S3: {filepath}",
                bucket=self.bucket_name,
                s3_key=s3_key,
                size_bytes=len(json_content.encode("utf-8")),
            )

            return f"s3://{self.bucket_name}/{s3_key}"

        except ClientError as e:
            logger.error(f"Failed to save JSON file to S3: {e}")
            raise

    def file_exists(self, filepath: str) -> bool:
        """Check if file exists in S3"""
        s3_key = self._get_s3_key(filepath)

        try:
            self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)
            return True
        except ClientError as e:
            if e.response["Error"]["Code"] == "404":
                return False
            raise

    def get_file_url(self, filepath: str) -> str:
        """Get S3 URL for the file"""
        s3_key = self._get_s3_key(filepath)
        return f"s3://{self.bucket_name}/{s3_key}"


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/main.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# app/main.py
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from datetime import datetime

from .config import settings
from .api.v1 import api_router
from .utils.exceptions import ExtractionError
from .utils.logging_manager import LoggingManager
from .services.schema_registry import SchemaRegistry

# Configure logging
LoggingManager.configure_logging(level=settings.log_level, debug=settings.debug)

logger = LoggingManager.get_logger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan events"""
    # Startup
    logger.info("Starting FastAPI Data Extractor API")

    # Initialize schema registry
    SchemaRegistry.discover_schemas()
    logger.success("Schema registry initialized")

    yield

    # Shutdown
    logger.info("Shutting down FastAPI Data Extractor API")


def create_app() -> FastAPI:
    """Create and configure FastAPI application"""

    app = FastAPI(
        title=settings.app_name,
        version=settings.version,
        description="Advanced data extraction API with multiple input types and configurable storage",
        lifespan=lifespan,
        docs_url="/docs",
        redoc_url="/redoc",
    )

    # Configure CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # Configure appropriately for production
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Include API routes
    app.include_router(api_router, prefix="/api")

    # Global exception handlers
    @app.exception_handler(ExtractionError)
    async def extraction_error_handler(request, exc: ExtractionError):
        logger.error(
            f"Extraction error: {str(exc)}",
            extraction_type=getattr(exc, "extraction_type", None),
            details=getattr(exc, "details", {}),
        )
        return JSONResponse(
            status_code=422,
            content={
                "detail": str(exc),
                "type": "extraction_error",
                "extraction_type": getattr(exc, "extraction_type", None),
            },
        )

    @app.exception_handler(ValueError)
    async def value_error_handler(request, exc: ValueError):
        logger.error(f"Validation error: {str(exc)}")
        return JSONResponse(
            status_code=400, content={"detail": str(exc), "type": "validation_error"}
        )

    @app.exception_handler(Exception)
    async def general_exception_handler(request, exc: Exception):
        logger.error(f"Unexpected error: {str(exc)}", error_type=type(exc).__name__)
        return JSONResponse(
            status_code=500,
            content={"detail": "Internal server error", "type": "internal_error"},
        )

    # Root endpoint
    @app.get("/")
    async def read_root():
        """Root endpoint with API information"""
        return {
            "message": "FastAPI Data Extractor API",
            "version": settings.version,
            "docs_url": "/docs",
            "health_check": "/health",
            "storage_backend": settings.storage_backend,
        }

    # Health check endpoint (also available at root level)
    @app.get("/health")
    async def health_check():
        """Health check endpoint for Docker and monitoring"""
        try:
            # Test storage backend connection
            from .core.storage.factory import StorageFactory

            storage = StorageFactory.create_storage()
            storage_status = "healthy"
        except Exception as e:
            storage_status = f"error: {str(e)}"

        return {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "version": settings.version,
            "storage_backend": settings.storage_backend,
            "storage_status": storage_status,
            "environment": "development" if settings.debug else "production",
        }

    return app


# Create app instance
app = create_app()

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.debug,
        log_level=settings.log_level.lower(),
    )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/base.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Any, Dict, Optional, Type
from pydantic import BaseModel
from enum import Enum


class InputType(str, Enum):
    TEXT = "text"
    URL = "url"
    IMAGE = "image"
    YOUTUBE_URL = "youtube_url"
    AUDIO = "audio"


class BaseExtractionSchema(BaseModel):
    """Base class for all extraction schemas

    Subclasses should define these class attributes:
    - extraction_type: str - Unique identifier for this extraction type
    - description: str - Human-readable description of what this schema extracts
    - prompt: str - System prompt for OpenAI extraction
    """

    # These will be set by subclasses as class attributes
    # Don't define them here to avoid Pydantic field conflicts
    pass


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/requests.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Optional
from pydantic import BaseModel, Field, field_validator
from .base import InputType


class ExtractionRequest(BaseModel):
    input_type: InputType = Field(
        ..., description="Type of input: text, url, image, audio, or youtube_url"
    )
    content: str = Field(
        ...,
        description="Text content, URL, image path/base64, audio path/base64, or YouTube URL to process",
    )
    extraction_type: str = Field(..., description="Type of data to extract")

    # File management options
    save_markdown: bool = Field(
        default=False, description="Save scraped markdown content"
    )
    save_json: bool = Field(default=True, description="Save extracted JSON data")
    output_directory: Optional[str] = Field(
        default=None, description="Custom output directory"
    )
    filename_prefix: Optional[str] = Field(
        default=None, description="Custom filename prefix"
    )

    # Processing options
    custom_instructions: Optional[str] = Field(
        default=None, description="Additional extraction instructions"
    )

    @field_validator("content")
    @classmethod
    def validate_content(cls, v):
        if not v or not v.strip():
            raise ValueError("Content cannot be empty")
        return v.strip()

    # COMMENTA questo validator che causa il problema
    # @validator('extraction_type')
    # def validate_extraction_type(cls, v):
    #     from ..services.schema_registry import SchemaRegistry
    #     # Validate against available schemas at runtime
    #     available_types = SchemaRegistry.get_available_types()
    #     if v not in available_types:
    #         raise ValueError(f"Invalid extraction type: {v}. Available: {available_types}")
    #     return v


class AudioUploadRequest(BaseModel):
    """Request model for audio file uploads"""

    extraction_type: str = Field(
        ..., description="Type of data to extract from transcription"
    )

    # File management options
    save_markdown: bool = Field(
        default=False, description="Save transcription as markdown"
    )
    save_json: bool = Field(default=True, description="Save extracted JSON data")
    output_directory: Optional[str] = Field(
        default=None, description="Custom output directory"
    )
    filename_prefix: Optional[str] = Field(
        default=None, description="Custom filename prefix"
    )

    # Processing options
    custom_instructions: Optional[str] = Field(
        default=None, description="Additional extraction instructions"
    )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/responses.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field
from datetime import datetime

class ExtractionResult(BaseModel):
    success: bool = Field(..., description="Whether extraction was successful")
    extraction_type: str = Field(..., description="Type of extraction performed")
    extracted_data: Optional[Dict[str, Any]] = Field(default=None, description="Extracted structured data")
    
    # File paths
    markdown_path: Optional[str] = Field(default=None, description="Path to saved markdown file")
    json_path: Optional[str] = Field(default=None, description="Path to saved JSON file")
    
    # Metadata
    processing_time: float = Field(..., description="Processing time in seconds")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    token_usage: Optional[Dict[str, int]] = Field(default=None, description="OpenAI token usage")
    
    # Error handling
    error_message: Optional[str] = Field(default=None, description="Error message if extraction failed")
    warnings: List[str] = Field(default_factory=list, description="Processing warnings")


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/schemas/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# Import only recipe schema
from .recipes import RecipeExtraction

__all__ = ["RecipeExtraction"]


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/schemas/recipes.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import List, Optional, ClassVar
from pydantic import BaseModel, Field
from ..base import BaseExtractionSchema


class Ingredient(BaseModel):
    name: str = Field(..., description="Ingredient name")
    quantity: Optional[str] = Field(default=None, description="Ingredient quantity")
    unit: Optional[str] = Field(default=None, description="Unit of measurement")


class CookingStep(BaseModel):
    step_number: int = Field(..., description="Step order number")
    instruction: str = Field(..., description="Cooking instruction")
    duration: Optional[str] = Field(
        default=None, description="Time required for this step"
    )


class RecipeExtraction(BaseExtractionSchema):
    title: str = Field(..., description="Recipe title")
    description: Optional[str] = Field(default=None, description="Recipe description")
    prep_time: Optional[str] = Field(default=None, description="Preparation time")
    cook_time: Optional[str] = Field(default=None, description="Cooking time")
    servings: Optional[int] = Field(default=None, description="Number of servings")

    ingredients: List[Ingredient] = Field(..., description="List of ingredients")
    instructions: List[CookingStep] = Field(..., description="Cooking instructions")

    # Class attributes - use ClassVar to tell Pydantic these are NOT fields
    extraction_type: ClassVar[str] = "recipes"
    description: ClassVar[str] = (
        "Extracts recipe information including ingredients and instructions"
    )
    prompt: ClassVar[str] = (
        "Extract recipe information from the given content. Focus on ingredients with quantities and step-by-step instructions."
    )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/services/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/services/extraction_service.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import time
from typing import Optional

from ..core.content_scraper import ContentScraper
from ..core.extraction_engine import ExtractionEngine
from ..core.file_manager import FileManager
from ..models.base import InputType
from ..models.requests import ExtractionRequest
from ..models.responses import ExtractionResult
from ..utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)


class ExtractionService:
    """Service for extraction requests supporting multiple input types"""

    def __init__(self):
        self.scraper = ContentScraper()
        self.extraction_engine = ExtractionEngine()
        self.file_manager = FileManager()

    async def process_extraction(self, request: ExtractionRequest) -> ExtractionResult:
        """Process extraction request with multiple input types"""
        start_time = time.time()

        logger.info(
            f"Processing extraction request",
            extraction_type=request.extraction_type,
            input_type=request.input_type.value,
        )

        try:
            # For images, use direct extraction if it's recipes
            if (
                request.input_type == InputType.IMAGE
                and request.extraction_type == "recipes"
            ):
                extracted_data, token_usage, extraction_time = (
                    self.extraction_engine.extract_from_image_directly(
                        request.content,
                        request.extraction_type,
                        request.custom_instructions,
                    )
                )
                markdown_path = None  # No intermediate text for direct extraction
                content = "Direct image extraction - no intermediate text"
            else:
                # Standard flow: extract text first, then process
                if request.input_type == InputType.TEXT:
                    content = request.content
                    markdown_path = None
                    if request.save_markdown:
                        markdown_path = self.file_manager.save_markdown(
                            content,
                            None,
                            request.output_directory,
                            request.filename_prefix,
                        )
                else:
                    # Use ContentScraper for URL, YOUTUBE_URL, AUDIO (now async!)
                    content, markdown_path = await self.scraper.fetch_content(
                        request.content,
                        request.input_type.value,
                        request.save_markdown,
                        request.output_directory,
                    )

                # Extract structured data
                extracted_data, token_usage, extraction_time = (
                    self.extraction_engine.extract_structured_data(
                        content, request.extraction_type, request.custom_instructions
                    )
                )

            # Save JSON if requested
            json_path = None
            if request.save_json:
                json_path = self.file_manager.save_json(
                    extracted_data,
                    content,
                    request.extraction_type,
                    request.output_directory,
                    request.filename_prefix,
                )

            processing_time = time.time() - start_time

            logger.success(
                f"Successfully completed extraction",
                extraction_type=request.extraction_type,
                total_time=round(processing_time, 2),
                extraction_time=round(extraction_time, 2),
            )

            return ExtractionResult(
                success=True,
                extraction_type=request.extraction_type,
                extracted_data=extracted_data,
                markdown_path=markdown_path,
                json_path=json_path,
                processing_time=processing_time,
                token_usage=token_usage,
            )

        except Exception as e:
            processing_time = time.time() - start_time
            error_message = str(e)

            # Log the FULL error with traceback
            logger.error(
                f"Extraction failed: {error_message}",
                extraction_type=request.extraction_type,
                error=error_message,
                processing_time=round(processing_time, 2),
                exc_info=True,  # This will include the full traceback
            )

            return ExtractionResult(
                success=False,
                extraction_type=request.extraction_type,
                processing_time=processing_time,
                error_message=error_message,
            )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/services/schema_registry.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# app/services/schema_registry.py
import importlib
import pkgutil
from typing import Dict, Type, List, Any
from pathlib import Path

from ..models.base import BaseExtractionSchema
from ..utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)

class SchemaRegistry:
    """Auto-discovery registry for extraction schemas"""
    
    _schemas: Dict[str, Type[BaseExtractionSchema]] = {}
    _discovered = False
    
    @classmethod
    def discover_schemas(cls) -> None:
        """Automatically discover all schema classes"""
        if cls._discovered:
            return
            
        logger.info("Discovering extraction schemas...")
        
        # Get the schemas package path
        schemas_path = Path(__file__).parent.parent / "models" / "schemas"
        
        # Import all modules in the schemas package
        for module_info in pkgutil.iter_modules([str(schemas_path)]):
            if module_info.name.startswith('_'):
                continue
                
            try:
                module_name = f"app.models.schemas.{module_info.name}"
                module = importlib.import_module(module_name)
                
                # Find all BaseExtractionSchema subclasses in the module
                for attr_name in dir(module):
                    attr = getattr(module, attr_name)
                    
                    if (isinstance(attr, type) and 
                        issubclass(attr, BaseExtractionSchema) and 
                        attr != BaseExtractionSchema and
                        hasattr(attr, 'extraction_type') and
                        attr.extraction_type):
                        
                        extraction_type = attr.extraction_type
                        cls._schemas[extraction_type] = attr
                        logger.info(f"Discovered schema: {extraction_type} -> {attr.__name__}")
                        
            except Exception as e:
                logger.warning(f"Failed to load schema module {module_info.name}: {e}")
        
        cls._discovered = True
        logger.info(f"Schema discovery complete. Found {len(cls._schemas)} schemas.")
    
    @classmethod
    def get_schema(cls, extraction_type: str) -> Type[BaseExtractionSchema]:
        """Get schema class for extraction type"""
        cls.discover_schemas()
        
        if extraction_type not in cls._schemas:
            available = list(cls._schemas.keys())
            raise ValueError(f"Unknown extraction type: {extraction_type}. Available: {available}")
        
        return cls._schemas[extraction_type]
    
    @classmethod
    def get_all_schemas(cls) -> Dict[str, Dict[str, Any]]:
        """Get information about all available schemas"""
        cls.discover_schemas()
        
        result = {}
        for extraction_type, schema_class in cls._schemas.items():
            result[extraction_type] = {
                "name": extraction_type,
                "description": schema_class.description,
                "prompt": schema_class.prompt,
                "schema": schema_class.model_json_schema(),
                "class_name": schema_class.__name__
            }
        return result
    
    @classmethod
    def get_available_types(cls) -> List[str]:
        """Get list of available extraction types"""
        cls.discover_schemas()
        return list(cls._schemas.keys())
    
    @classmethod
    def reload_schemas(cls) -> None:
        """Force reload of all schemas (useful for development)"""
        cls._schemas.clear()
        cls._discovered = False
        cls.discover_schemas()

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/utils/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/utils/exceptions.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
"""Custom exceptions for the structured extraction API"""

class ExtractionError(Exception):
    """Base exception for extraction-related errors"""
    
    def __init__(self, message: str, extraction_type: str = None, details: dict = None):
        self.message = message
        self.extraction_type = extraction_type
        self.details = details or {}
        super().__init__(self.message)

class SchemaNotFoundError(ExtractionError):
    """Raised when requested extraction schema is not found"""
    
    def __init__(self, extraction_type: str, available_types: list = None):
        self.available_types = available_types or []
        message = f"Schema not found: {extraction_type}"
        if self.available_types:
            message += f". Available types: {', '.join(self.available_types)}"
        super().__init__(message, extraction_type)

class ContentScrapingError(ExtractionError):
    """Raised when content scraping fails"""
    
    def __init__(self, url: str, reason: str = None):
        self.url = url
        self.reason = reason
        message = f"Failed to scrape content from URL: {url}"
        if reason:
            message += f". Reason: {reason}"
        super().__init__(message, details={"url": url, "reason": reason})

class OpenAIExtractionError(ExtractionError):
    """Raised when OpenAI extraction fails"""
    
    def __init__(self, message: str, extraction_type: str = None, token_usage: dict = None):
        self.token_usage = token_usage or {}
        details = {"token_usage": self.token_usage}
        super().__init__(message, extraction_type, details)

class ValidationError(ExtractionError):
    """Raised when input validation fails"""
    
    def __init__(self, field: str, value: str, reason: str):
        self.field = field
        self.value = value
        self.reason = reason
        message = f"Validation error for field '{field}': {reason}"
        details = {"field": field, "value": value, "reason": reason}
        super().__init__(message, details=details)

class ConfigurationError(ExtractionError):
    """Raised when there's a configuration issue"""
    
    def __init__(self, setting: str, message: str):
        self.setting = setting
        full_message = f"Configuration error for '{setting}': {message}"
        details = {"setting": setting}
        super().__init__(full_message, details=details)

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/utils/logging_manager.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import sys
from typing import Optional
from loguru import logger


class LoggingManager:
    """
    Centralized logging manager using Loguru
    Provides structured logging with consistent formatting
    """
    
    _configured = False
    
    @classmethod
    def configure_logging(cls, level: str = "INFO", debug: bool = False) -> None:
        """Configure loguru logger with appropriate settings"""
        if cls._configured:
            return
            
        # Remove default handler
        logger.remove()
        
        # Configure log level based on debug mode
        log_level = "DEBUG" if debug else level
        
        # Add console handler with structured format
        logger.add(
            sys.stdout,
            level=log_level,
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
                   "<level>{level: <8}</level> | "
                   "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
                   "<level>{message}</level>",
            colorize=True,
            diagnose=debug
        )
        
        # Add file handler for errors
        logger.add(
            "logs/error.log",
            level="ERROR",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}",
            rotation="10 MB",
            retention="30 days",
            compression="zip"
        )
        
        # Add file handler for all logs if debug
        if debug:
            logger.add(
                "logs/debug.log",
                level="DEBUG",
                format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}",
                rotation="50 MB",
                retention="7 days",
                compression="zip"
            )
        
        cls._configured = True
        logger.info("Logging configured successfully")
    
    @classmethod
    def get_logger(cls, name: Optional[str] = None):
        """Get a logger instance with optional name binding"""
        if not cls._configured:
            cls.configure_logging()
        
        if name:
            return logger.bind(name=name)
        return logger
    
    @classmethod
    def add_file_handler(cls, file_path: str, level: str = "INFO", 
                        rotation: str = "10 MB", retention: str = "30 days") -> None:
        """Add additional file handler"""
        logger.add(
            file_path,
            level=level,
            format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}",
            rotation=rotation,
            retention=retention,
            compression="zip"
        )
        logger.info(f"Added file handler: {file_path}")
    
    @classmethod
    def set_level(cls, level: str) -> None:
        """Change logging level dynamically"""
        # This would require more complex implementation to change existing handlers
        logger.info(f"Logging level change requested: {level}") 

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: .python-version
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
3.13


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: codebase_context.txt
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
================================================================================
CODEBASE CONTEXT FOR LLM
================================================================================
Generated: 2025-06-22 16:39:48
Root Path: /home/sav/projects/fastapi-data-extractor
Excluded: docker/, data/, .gitignore, test_safe_storage.py, README.md, .dockerignore, .env files, cache, logs
Included: source code, configuration files (pyproject.toml, etc.)
================================================================================

📁 REPOSITORY STRUCTURE
==================================================
Complete directory tree showing all relevant files and folders:

fastapi-data-extractor/
├── app
│   ├── api
│   │   ├── v1
│   │   │   ├── modules
│   │   │   │   ├── __init__.py
│   │   │   │   └── kitchen.py
│   │   │   ├── __init__.py
│   │   │   └── router.py
│   │   └── __init__.py
│   ├── core
│   │   ├── storage
│   │   │   ├── __init__.py
│   │   │   ├── base.py
│   │   │   ├── factory.py
│   │   │   ├── local_storage.py
│   │   │   └── s3_storage.py
│   │   ├── __init__.py
│   │   ├── content_scraper.py
│   │   ├── extraction_engine.py
│   │   └── file_manager.py
│   ├── models
│   │   ├── schemas
│   │   │   ├── __init__.py
│   │   │   └── recipes.py
│   │   ├── __init__.py
│   │   ├── base.py
│   │   ├── requests.py
│   │   └── responses.py
│   ├── services
│   │   ├── __init__.py
│   │   ├── extraction_service.py
│   │   └── schema_registry.py
│   ├── utils
│   │   ├── __init__.py
│   │   ├── exceptions.py
│   │   └── logging_manager.py
│   ├── __init__.py
│   ├── config.py
│   └── main.py
├── codebase_context.txt
└── pyproject.toml

==================================================

📄 SOURCE CODE AND CONFIGURATION FILES
==================================================

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: pyproject.toml
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
[project]
name = "fastapi-data-extractor"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "fastapi[standard]>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "openai>=1.54.0",
    "pydantic>=2.10.0",
    "pydantic-settings>=2.6.0",
    "playwright>=1.49.0",
    "html2text>=2024.2.26",
    "logfire>=0.54.0",
    "python-multipart>=0.0.12",
    "aiofiles>=24.1.0",
    "loguru>=0.7.2",
    "youtube-transcript-api>=1.1.0",
    "yt-dlp>=2025.6.9",
    "boto3>=1.35.0",
]


[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.24.0",
    "httpx>=0.28.0",
    "black>=24.0.0",
    "ruff>=0.8.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["app"]

[tool.uv]
dev-dependencies = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.24.0",
    "httpx>=0.28.0",
    "black>=24.0.0",
    "ruff>=0.8.0",
]


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/v1/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from .router import api_router

__all__ = ["api_router"]


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/v1/modules/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# Domain modules package


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/v1/modules/kitchen.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Dict, Any
from fastapi import APIRouter, HTTPException, Depends, UploadFile, File, Form
from ....models.requests import ExtractionRequest, AudioUploadRequest
from ....models.responses import ExtractionResult
from ....services.extraction_service import ExtractionService
from ....utils.logging_manager import LoggingManager
import base64
import tempfile
import os

logger = LoggingManager.get_logger(__name__)


router = APIRouter()


def get_extraction_service() -> ExtractionService:
    """Dependency to get extraction service instance"""
    return ExtractionService()


@router.post("/extract-recipe", response_model=ExtractionResult)
async def extract_recipe_data(
    request: ExtractionRequest,
    service: ExtractionService = Depends(get_extraction_service),
) -> ExtractionResult:
    """
    Extract structured recipe data from multiple input types:
    - text: Direct text input
    - url: Web page URL
    - image: Image file (OCR)
    - audio: Audio file (transcription)
    - youtube_url: YouTube video (transcript)
    """
    try:
        request.extraction_type = "recipes"
        logger.info(f"Processing recipe extraction from {request.input_type.value}")
        return await service.process_extraction(request)
    except Exception as e:
        logger.error(f"Recipe extraction failed: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Recipe extraction failed: {str(e)}"
        )


@router.post("/extract-ingredients", response_model=ExtractionResult)
async def extract_ingredients_data(
    request: ExtractionRequest,
    service: ExtractionService = Depends(get_extraction_service),
) -> ExtractionResult:
    """Extract ingredients list from recipe content"""
    try:
        request.extraction_type = "ingredients"
        logger.info("Processing ingredients extraction request")
        return await service.process_extraction(request)
    except Exception as e:
        logger.error(f"Ingredients extraction failed: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Ingredients extraction failed: {str(e)}"
        )


@router.post("/extract-recipe-from-file", response_model=ExtractionResult)
async def extract_recipe_from_file(
    file: UploadFile = File(...),
    service: ExtractionService = Depends(get_extraction_service),
) -> ExtractionResult:
    """Upload an image file and extract recipe data"""
    try:
        # Read file content
        file_content = await file.read()
        base64_image = base64.b64encode(file_content).decode("utf-8")

        # Create request
        request = ExtractionRequest(
            input_type="image",
            content=f"data:image/{file.content_type.split('/')[-1]};base64,{base64_image}",
            extraction_type="recipes",
            save_json=True,
        )

        return await service.process_extraction(request)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/extract-recipe-from-audio", response_model=ExtractionResult)
async def extract_recipe_from_audio(
    audio_file: UploadFile = File(..., description="Audio file to transcribe"),
    save_markdown: bool = Form(
        default=False, description="Save transcription as markdown"
    ),
    save_json: bool = Form(default=True, description="Save extracted JSON data"),
    output_directory: str = Form(default=None, description="Custom output directory"),
    filename_prefix: str = Form(default=None, description="Custom filename prefix"),
    custom_instructions: str = Form(
        default=None, description="Additional extraction instructions"
    ),
    service: ExtractionService = Depends(get_extraction_service),
) -> ExtractionResult:
    """Upload an audio file, transcribe it using OpenAI Whisper, and extract recipe data"""
    temp_file_path = None
    try:
        # Validate audio file type
        if not audio_file.content_type.startswith("audio/"):
            raise HTTPException(
                status_code=400,
                detail=f"Invalid file type. Expected audio file, got {audio_file.content_type}",
            )

        # Save uploaded file to temporary location
        file_content = await audio_file.read()

        # Get file extension from content type or filename
        file_extension = ".wav"  # Default
        if audio_file.content_type == "audio/mpeg":
            file_extension = ".mp3"
        elif audio_file.content_type == "audio/wav":
            file_extension = ".wav"
        elif audio_file.content_type == "audio/m4a":
            file_extension = ".m4a"
        elif audio_file.content_type == "audio/ogg":
            file_extension = ".ogg"
        elif audio_file.filename:
            # Extract from filename if available
            original_ext = os.path.splitext(audio_file.filename)[1]
            if original_ext in [".mp3", ".wav", ".m4a", ".ogg", ".webm"]:
                file_extension = original_ext

        # Create temporary file
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=file_extension)
        temp_file.write(file_content)
        temp_file.close()
        temp_file_path = temp_file.name

        logger.info(
            f"Processing audio file: {audio_file.filename} ({audio_file.content_type})"
        )

        # Create extraction request
        request = ExtractionRequest(
            input_type="audio",
            content=temp_file_path,
            extraction_type="recipes",
            save_markdown=save_markdown,
            save_json=save_json,
            output_directory=output_directory,
            filename_prefix=filename_prefix,
            custom_instructions=custom_instructions,
        )

        # Process extraction
        result = await service.process_extraction(request)

        logger.success(f"Successfully processed audio file: {audio_file.filename}")
        return result

    except Exception as e:
        logger.error(f"Audio recipe extraction failed: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Audio recipe extraction failed: {str(e)}"
        )
    finally:
        # Clean up temporary file
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.unlink(temp_file_path)
            except Exception as e:
                logger.warning(
                    f"Failed to clean up temporary file {temp_file_path}: {str(e)}"
                )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/api/v1/router.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from fastapi import APIRouter
from .modules import kitchen

api_router = APIRouter()

# Only kitchen module with single tag
api_router.include_router(
    kitchen.router, prefix="/kitchen", tags=["kitchen"]  # ← Solo un tag = una sezione
)


@api_router.get("/modules")
async def get_available_modules():
    """Get information about available modules"""
    return {"available_modules": ["kitchen"], "total_modules": 1}


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/config.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import os
from pathlib import Path
from pydantic_settings import BaseSettings
from typing import Literal


class Settings(BaseSettings):
    # API Configuration
    app_name: str = "Structured Extraction API"
    version: str = "0.1.0"
    debug: bool = True

    # OpenAI Configuration
    openai_api_key: str
    openai_model: str = "gpt-4o-2024-08-06"

    # Storage Configuration
    storage_backend: Literal["local", "s3"] = "local"
    data_dir: str = "data"
    markdown_output_dir: str = "data/markdown"
    json_output_dir: str = "data/json"

    # S3 Configuration (only used if storage_backend is "s3")
    s3_bucket_name: str = ""
    s3_region: str = "eu-west-3"
    s3_access_key_id: str = ""
    s3_secret_access_key: str = ""
    s3_endpoint_url: str = ""  # For custom S3-compatible services
    s3_prefix: str = "fastapi-data-extractor/"  # Prefix for all files in S3

    # Scraper Configuration
    playwright_headless: bool = True
    playwright_timeout: int = 60000

    # Logging Configuration
    log_level: str = "INFO"
    log_to_file: bool = True
    log_file_path: str = "logs/app.log"
    log_rotation: str = "10 MB"
    log_retention: str = "1 month"
    log_format: str = (
        "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}"
    )

    # File Management
    enable_versioning: bool = True
    max_file_size_mb: int = 10

    class Config:
        env_file = ".env"
        case_sensitive = False


settings = Settings()


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/content_scraper.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# app/core/content_scraper.py
import asyncio
import hashlib
import os
from pathlib import Path
from typing import Optional, Tuple
import tempfile
import base64

import html2text
from playwright.async_api import async_playwright

from ..config import settings
from ..utils.logging_manager import LoggingManager
from ..core.file_manager import FileManager

from youtube_transcript_api import YouTubeTranscriptApi
import re
from urllib.parse import urlparse, parse_qs
from openai import OpenAI

logger = LoggingManager.get_logger(__name__)


class ContentScraper:
    """Enhanced content scraper supporting multiple input types"""

    def __init__(self):
        self.file_manager = FileManager()
        self.openai_client = OpenAI(api_key=settings.openai_api_key)

    async def fetch_content(
        self,
        content: str,
        input_type: str,
        save_markdown: bool = False,
        output_directory: Optional[str] = None,
    ) -> Tuple[str, Optional[str]]:
        """
        Extract text content from various input types

        Returns:
            Tuple of (extracted_text, markdown_path)
        """
        try:
            if input_type == "text":
                return self._process_text(content, save_markdown, output_directory)
            elif input_type == "url":
                return await self._process_url_async(
                    content, save_markdown, output_directory
                )
            elif input_type == "image":
                return self._process_image_with_vision(
                    content, save_markdown, output_directory
                )
            elif input_type == "youtube_url":
                return self._process_youtube(content, save_markdown, output_directory)
            elif input_type == "audio":
                return self._process_audio_with_whisper(
                    content, save_markdown, output_directory
                )
            else:
                raise ValueError(f"Unsupported input type: {input_type}")

        except Exception as e:
            logger.error(f"Content extraction failed for {input_type}: {str(e)}")
            raise

    def _process_text(
        self, text: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process plain text input"""
        markdown_path = None
        if save_markdown:
            markdown_path = self.file_manager.save_markdown(
                text, None, output_dir, "text_input"
            )
        return text, markdown_path

    async def _process_url_async(
        self, url: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process URL input using web scraping"""
        logger.info(f"Processing URL: {url}")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            try:
                # Navigate to URL
                await page.goto(url, wait_until="networkidle")

                # Get page content
                html_content = await page.content()

                # Convert HTML to markdown
                h = html2text.HTML2Text()
                h.ignore_links = False
                h.ignore_images = True
                markdown_content = h.handle(html_content)

                # Clean up content
                content = markdown_content.strip()

                if not content:
                    raise ValueError("No content extracted from URL")

                logger.info(f"URL scraping extracted {len(content)} characters")

                # Save markdown if requested
                markdown_path = None
                if save_markdown:
                    # Generate filename from URL
                    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
                    markdown_path = self.file_manager.save_markdown(
                        content, url, output_dir, f"url_{url_hash}"
                    )

                return content, markdown_path

            except Exception as e:
                logger.error(f"URL scraping failed: {str(e)}")
                raise
            finally:
                await browser.close()

    def _process_image_with_vision(
        self, image_input: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process image using OpenAI Vision API - no PIL needed!"""
        logger.info("Processing image with OpenAI Vision API")

        try:
            # Prepare image for OpenAI (no PIL processing)
            image_url = self._prepare_image_for_openai(image_input)

            # Direct API call - no local image processing
            response = self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Extract all text content from this image. If it contains a recipe, cooking instructions, or food-related content, provide a detailed transcription.",
                            },
                            {"type": "image_url", "image_url": {"url": image_url}},
                        ],
                    }
                ],
                max_tokens=1500,
            )

            extracted_text = response.choices[0].message.content

            if not extracted_text or not extracted_text.strip():
                raise ValueError("No content extracted from image")

            logger.info(f"Vision API extracted {len(extracted_text)} characters")

            # Save markdown if requested
            markdown_path = None
            if save_markdown:
                markdown_path = self.file_manager.save_markdown(
                    extracted_text,
                    f"image_vision_{hash(image_input)}",
                    output_dir,
                    "image_vision",
                )

            return extracted_text.strip(), markdown_path

        except Exception as e:
            logger.error(f"Vision API processing failed: {str(e)}")
            raise

    def _prepare_image_for_openai(self, image_input: str) -> str:
        """Prepare image for OpenAI Vision API - no PIL needed!"""

        # URL? Pass through
        if image_input.startswith("http"):
            return image_input

        # Already base64? Pass through
        if image_input.startswith("data:image"):
            return image_input

        # File path? Convert to base64 (no PIL needed)
        if image_input.startswith("/") or image_input.startswith("./"):
            with open(image_input, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode("utf-8")
                # Simple extension detection
                ext = image_input.lower().split(".")[-1]
                mime_type = (
                    f"image/{ext}"
                    if ext in ["jpg", "jpeg", "png", "gif", "webp"]
                    else "image/jpeg"
                )
                return f"data:{mime_type};base64,{base64_image}"

        raise ValueError("Invalid image input format")

    def _process_youtube(
        self, youtube_url: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process YouTube URL to extract transcript"""
        logger.info(f"Processing YouTube URL: {youtube_url}")

        try:
            # Extract video ID from URL
            video_id = self._extract_youtube_id(youtube_url)
            if not video_id:
                raise ValueError("Invalid YouTube URL")

            # Get transcript
            transcript_list = YouTubeTranscriptApi.get_transcript(
                video_id,
                languages=[
                    "en",
                    "it",
                    "auto",
                ],  # Try English, Italian, then auto-generated
            )

            # Combine transcript text
            transcript_text = " ".join([entry["text"] for entry in transcript_list])

            if not transcript_text.strip():
                raise ValueError("No transcript available for this video")

            logger.info(
                f"YouTube transcript extracted: {len(transcript_text)} characters"
            )

            # Save markdown if requested
            markdown_path = None
            if save_markdown:
                markdown_path = self.file_manager.save_markdown(
                    transcript_text, youtube_url, output_dir, f"youtube_{video_id}"
                )

            return transcript_text.strip(), markdown_path

        except Exception as e:
            logger.error(f"YouTube transcript extraction failed: {str(e)}")
            raise

    def _extract_youtube_id(self, url: str) -> Optional[str]:
        """Extract video ID from YouTube URL"""
        patterns = [
            r"(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)",
            r"youtube\.com\/watch\?.*v=([^&\n?#]+)",
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        return None

    def _process_audio_with_whisper(
        self, audio_input: str, save_markdown: bool, output_dir: Optional[str]
    ) -> Tuple[str, Optional[str]]:
        """Process audio file using OpenAI Whisper API"""
        logger.info("Processing audio with OpenAI Whisper API")

        try:
            # Prepare audio file for OpenAI
            audio_file_path = self._prepare_audio_for_openai(audio_input)

            # Transcribe audio using Whisper
            with open(audio_file_path, "rb") as audio_file:
                transcript = self.openai_client.audio.transcriptions.create(
                    model="whisper-1", file=audio_file, response_format="text"
                )

            if not transcript or not transcript.strip():
                raise ValueError("No content extracted from audio")

            logger.info(f"Whisper API transcribed {len(transcript)} characters")

            # Save markdown if requested
            markdown_path = None
            if save_markdown:
                audio_hash = hashlib.md5(audio_input.encode()).hexdigest()[:8]
                markdown_path = self.file_manager.save_markdown(
                    transcript,
                    f"audio_transcription_{audio_hash}",
                    output_dir,
                    "audio_transcription",
                )

            # Clean up temporary file if created
            if audio_file_path != audio_input and os.path.exists(audio_file_path):
                os.unlink(audio_file_path)

            return transcript.strip(), markdown_path

        except Exception as e:
            logger.error(f"Whisper API processing failed: {str(e)}")
            raise

    def _prepare_audio_for_openai(self, audio_input: str) -> str:
        """Prepare audio for OpenAI Whisper API"""

        # If it's a file path, return it directly
        if os.path.isfile(audio_input):
            return audio_input

        # If it's base64 encoded audio, decode and save to temp file
        if audio_input.startswith("data:audio"):
            # Extract base64 data
            header, data = audio_input.split(",", 1)
            audio_data = base64.b64decode(data)

            # Extract file extension from header
            if "audio/wav" in header:
                ext = ".wav"
            elif "audio/mp3" in header:
                ext = ".mp3"
            elif "audio/m4a" in header:
                ext = ".m4a"
            elif "audio/ogg" in header:
                ext = ".ogg"
            else:
                ext = ".wav"  # Default

            # Create temporary file
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=ext)
            temp_file.write(audio_data)
            temp_file.close()

            return temp_file.name

        # If it's raw base64 without header, assume it's audio
        try:
            audio_data = base64.b64decode(audio_input)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
            temp_file.write(audio_data)
            temp_file.close()
            return temp_file.name
        except Exception:
            pass

        raise ValueError(
            "Invalid audio input format. Provide file path or base64 encoded audio."
        )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/extraction_engine.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# app/core/extraction_engine.py
import time
from typing import Optional, Dict, Any, Tuple
from openai import OpenAI

from ..config import settings
from ..services.schema_registry import SchemaRegistry
from ..utils.logging_manager import LoggingManager
from ..core.content_scraper import ContentScraper

logger = LoggingManager.get_logger(__name__)


class ExtractionEngine:
    """Core engine for structured data extraction using OpenAI"""

    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        self.api_key = api_key or settings.openai_api_key
        if not self.api_key:
            raise ValueError(
                "OpenAI API key is required. Provide it as parameter or set "
                "OPENAI_API_KEY environment variable."
            )

        self.client = OpenAI(api_key=self.api_key)
        self.model = model or settings.openai_model
        self.schema_registry = SchemaRegistry()

    def extract_structured_data(
        self,
        content: str,
        extraction_type: str,
        custom_instructions: Optional[str] = None,
    ) -> Tuple[Dict[str, Any], Dict[str, int], float]:
        """
        Extract structured data from content using specified schema

        Returns:
            Tuple of (extracted_data, token_usage, processing_time)
        """
        start_time = time.time()

        # Try registry first, fallback to direct import
        try:
            schema_class = self.schema_registry.get_schema(extraction_type)
        except ValueError:
            # Direct import fallback for new schemas
            if extraction_type == "recipes":
                from ..models.schemas.recipes import RecipeExtraction

                schema_class = RecipeExtraction
            elif extraction_type == "quotes":
                from ..models.schemas.quotes import QuotesExtraction

                schema_class = QuotesExtraction
            elif extraction_type == "insights":
                from ..models.schemas.insights import InsightsExtraction

                schema_class = InsightsExtraction
            else:
                raise ValueError(f"Unknown extraction type: {extraction_type}")

        # Build system prompt
        system_prompt = schema_class.prompt
        if custom_instructions:
            system_prompt += f"\n\nAdditional instructions: {custom_instructions}"

        logger.info(f"Starting extraction", extraction_type=extraction_type)

        try:
            completion = self.client.beta.chat.completions.parse(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user",
                        "content": (
                            f"Extract structured information from this content:\n\n{content}"
                        ),
                    },
                ],
                response_format=schema_class,
            )

            result = completion.choices[0].message.parsed
            token_usage = {
                "prompt_tokens": completion.usage.prompt_tokens,
                "completion_tokens": completion.usage.completion_tokens,
                "total_tokens": completion.usage.total_tokens,
            }

            processing_time = time.time() - start_time

            logger.success(
                f"Successfully extracted data",
                extraction_type=extraction_type,
                processing_time=round(processing_time, 2),
                tokens_used=token_usage["total_tokens"],
            )

            return result.model_dump(), token_usage, processing_time

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(
                f"Error extracting structured data",
                extraction_type=extraction_type,
                error=str(e),
                processing_time=round(processing_time, 2),
            )

            raise

    def extract_from_image_directly(
        self,
        image_input: str,
        extraction_type: str,
        custom_instructions: Optional[str] = None,
    ) -> Tuple[Dict[str, Any], Dict[str, int], float]:
        """Extract structured data directly from image using Vision API"""
        start_time = time.time()

        # Get schema class
        if extraction_type == "recipes":
            from ..models.schemas.recipes import RecipeExtraction

            schema_class = RecipeExtraction
        else:
            raise ValueError(
                f"Direct image extraction not supported for: {extraction_type}"
            )

        # Prepare image
        scraper = ContentScraper()
        image_url = scraper._prepare_image_for_openai(image_input)

        # Build prompt for direct extraction
        system_prompt = f"""You are extracting structured recipe data directly from an image. 
        {schema_class.prompt}
        
        Look at the image carefully and extract all recipe information you can see.
        If ingredients are listed, extract them with quantities.
        If cooking steps are shown, extract them in order.
        """

        if custom_instructions:
            system_prompt += f"\n\nAdditional instructions: {custom_instructions}"

        try:
            completion = self.client.beta.chat.completions.parse(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": "Extract recipe information from this image:",
                            },
                            {"type": "image_url", "image_url": {"url": image_url}},
                        ],
                    },
                ],
                response_format=schema_class,
            )

            result = completion.choices[0].message.parsed
            token_usage = {
                "prompt_tokens": completion.usage.prompt_tokens,
                "completion_tokens": completion.usage.completion_tokens,
                "total_tokens": completion.usage.total_tokens,
            }

            processing_time = time.time() - start_time

            logger.success(
                f"Direct image extraction completed",
                processing_time=round(processing_time, 2),
            )

            return result.model_dump(), token_usage, processing_time

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Direct image extraction failed: {str(e)}")
            raise


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/file_manager.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import hashlib
from datetime import datetime, timezone
from typing import Dict, Any, Optional

from ..config import settings
from ..utils.logging_manager import LoggingManager
from .storage.factory import StorageFactory
from .storage.base import StorageBackend

logger = LoggingManager.get_logger(__name__)


class FileManager:
    """Manages file operations for extracted data with configurable storage backends"""

    def __init__(self, storage_backend: Optional[StorageBackend] = None):
        self.storage = storage_backend or StorageFactory.create_storage()

    def _generate_filename(
        self,
        content_hash: str,
        extraction_type: str,
        prefix: Optional[str] = None,
        extension: str = ".json",
    ) -> str:
        """Generate filename with optional versioning"""
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")

        parts = []
        if prefix:
            parts.append(prefix)
        parts.extend([extraction_type, content_hash[:8], timestamp])

        return "_".join(parts) + extension

    def _get_content_hash(self, content: str) -> str:
        """Generate hash for content"""
        return hashlib.sha256(content.encode()).hexdigest()

    def save_json(
        self,
        data: Dict[str, Any],
        content: str,
        extraction_type: str,
        output_dir: Optional[str] = None,
        filename_prefix: Optional[str] = None,
    ) -> str:
        """Save extracted data as JSON file"""
        content_hash = self._get_content_hash(content)
        filename = self._generate_filename(
            content_hash, extraction_type, filename_prefix
        )

        # Determine the relative path for storage
        if output_dir:
            # Extract just the directory name if full path provided
            dir_name = output_dir.split("/")[-1] if "/" in output_dir else output_dir
            filepath = f"{dir_name}/{filename}"
        else:
            filepath = f"json/{filename}"

        # Prepare data with metadata
        save_data = {
            "metadata": {
                "extraction_type": extraction_type,
                "content_hash": content_hash,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "filename": filename,
                "storage_backend": settings.storage_backend,
            },
            "extracted_data": data,
        }

        # Save using the storage backend
        saved_path = self.storage.save_json(save_data, filepath)

        logger.info(
            f"Saved JSON file: {filename}",
            filepath=saved_path,
            extraction_type=extraction_type,
            storage_backend=settings.storage_backend,
        )

        return saved_path

    def save_markdown(
        self,
        content: str,
        source_url: Optional[str] = None,
        output_dir: Optional[str] = None,
        filename_prefix: Optional[str] = None,
    ) -> str:
        """Save markdown content to file"""
        content_hash = self._get_content_hash(content)
        filename = self._generate_filename(
            content_hash, "markdown", filename_prefix, ".md"
        )

        # Determine the relative path for storage
        if output_dir:
            # Extract just the directory name if full path provided
            dir_name = output_dir.split("/")[-1] if "/" in output_dir else output_dir
            filepath = f"{dir_name}/{filename}"
        else:
            filepath = f"markdown/{filename}"

        # Prepare markdown content with metadata
        markdown_content = ""
        if source_url:
            markdown_content += f"<!-- Source URL: {source_url} -->\n"
        markdown_content += f"<!-- Content Hash: {content_hash} -->\n"
        markdown_content += (
            f"<!-- Saved: {datetime.now(timezone.utc).isoformat()} -->\n"
        )
        markdown_content += f"<!-- Storage Backend: {settings.storage_backend} -->\n\n"
        markdown_content += content

        # Save using the storage backend
        saved_path = self.storage.save_file(
            markdown_content,
            filepath,
            metadata={
                "source_url": source_url or "",
                "content_hash": content_hash,
                "storage_backend": settings.storage_backend,
            },
        )

        logger.info(
            f"Saved markdown file: {filename}",
            filepath=saved_path,
            storage_backend=settings.storage_backend,
        )

        return saved_path


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# Storage module initialization
from .base import StorageBackend
from .local_storage import LocalStorage
from .s3_storage import S3Storage
from .factory import StorageFactory

__all__ = ["StorageBackend", "LocalStorage", "S3Storage", "StorageFactory"]


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/base.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional


class StorageBackend(ABC):
    """Abstract base class for storage backends"""

    @abstractmethod
    def save_file(
        self, content: str, filepath: str, metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Save content to a file

        Args:
            content: Content to save
            filepath: Path where to save the file
            metadata: Optional metadata to include

        Returns:
            Full path or URL of the saved file
        """
        pass

    @abstractmethod
    def save_json(
        self,
        data: Dict[str, Any],
        filepath: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Save JSON data to a file

        Args:
            data: JSON data to save
            filepath: Path where to save the file
            metadata: Optional metadata to include

        Returns:
            Full path or URL of the saved file
        """
        pass

    @abstractmethod
    def file_exists(self, filepath: str) -> bool:
        """Check if a file exists"""
        pass

    @abstractmethod
    def get_file_url(self, filepath: str) -> str:
        """Get the URL/path to access the file"""
        pass


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/factory.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Optional

from .base import StorageBackend
from .local_storage import LocalStorage
from .s3_storage import S3Storage
from ...config import settings
from ...utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)


class StorageFactory:
    """Factory class to create storage backends"""

    @staticmethod
    def create_storage(backend_type: Optional[str] = None) -> StorageBackend:
        """
        Create a storage backend based on configuration

        Args:
            backend_type: Override the configured backend type

        Returns:
            StorageBackend instance
        """
        backend = backend_type or settings.storage_backend

        if backend == "local":
            logger.info("Using local storage backend")
            return LocalStorage(base_dir=settings.data_dir)

        elif backend == "s3":
            logger.info("Using S3 storage backend")

            # Validate S3 configuration
            if not settings.s3_bucket_name:
                raise ValueError("S3 bucket name is required when using S3 storage")

            return S3Storage(
                bucket_name=settings.s3_bucket_name,
                region=settings.s3_region,
                access_key_id=settings.s3_access_key_id or None,
                secret_access_key=settings.s3_secret_access_key or None,
                endpoint_url=settings.s3_endpoint_url or None,
                prefix=settings.s3_prefix,
            )

        else:
            raise ValueError(f"Unsupported storage backend: {backend}")


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/local_storage.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import json
import os
from pathlib import Path
from typing import Dict, Any, Optional

from .base import StorageBackend
from ...utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)


class LocalStorage(StorageBackend):
    """Local filesystem storage backend"""

    def __init__(self, base_dir: str = "data"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)

    def save_file(
        self, content: str, filepath: str, metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Save content to local file"""
        full_path = self.base_dir / filepath
        full_path.parent.mkdir(parents=True, exist_ok=True)

        with open(full_path, "w", encoding="utf-8") as f:
            f.write(content)

        file_size = os.path.getsize(full_path)
        logger.info(
            f"Saved file locally: {filepath}",
            filepath=str(full_path),
            size_bytes=file_size,
        )

        return str(full_path)

    def save_json(
        self,
        data: Dict[str, Any],
        filepath: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Save JSON data to local file"""
        full_path = self.base_dir / filepath
        full_path.parent.mkdir(parents=True, exist_ok=True)

        save_data = data.copy()
        if metadata:
            save_data["metadata"] = {**save_data.get("metadata", {}), **metadata}

        with open(full_path, "w", encoding="utf-8") as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)

        file_size = os.path.getsize(full_path)
        logger.info(
            f"Saved JSON file locally: {filepath}",
            filepath=str(full_path),
            size_bytes=file_size,
        )

        return str(full_path)

    def file_exists(self, filepath: str) -> bool:
        """Check if file exists locally"""
        return (self.base_dir / filepath).exists()

    def get_file_url(self, filepath: str) -> str:
        """Get local file path"""
        return str(self.base_dir / filepath)


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/core/storage/s3_storage.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import json
from typing import Dict, Any, Optional
from datetime import datetime, timezone

try:
    import boto3
    from botocore.exceptions import ClientError, NoCredentialsError

    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False

from .base import StorageBackend
from ...utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)


class S3Storage(StorageBackend):
    """S3 storage backend"""

    def __init__(
        self,
        bucket_name: str,
        region: str = "us-east-1",
        access_key_id: Optional[str] = None,
        secret_access_key: Optional[str] = None,
        endpoint_url: Optional[str] = None,
        prefix: str = "",
    ):
        if not BOTO3_AVAILABLE:
            raise ImportError(
                "boto3 is required for S3 storage. Install it with: pip install boto3"
            )

        self.bucket_name = bucket_name
        self.prefix = prefix.rstrip("/") + "/" if prefix else ""

        # Initialize S3 client
        # Se access_key_id e secret_access_key sono None o vuoti, boto3 userà le credenziali di default
        if access_key_id and secret_access_key:
            # Usa credenziali esplicite
            session = boto3.Session(
                aws_access_key_id=access_key_id,
                aws_secret_access_key=secret_access_key,
                region_name=region,
            )
            logger.info("Using explicit AWS credentials from environment")
        else:
            # Usa credenziali di default (aws configure, IAM role, etc.)
            session = boto3.Session(region_name=region)
            logger.info("Using default AWS credentials (aws configure or IAM role)")

        client_kwargs = {}
        if endpoint_url:
            client_kwargs["endpoint_url"] = endpoint_url

        self.s3_client = session.client("s3", **client_kwargs)

        # Verify bucket access
        self._verify_bucket_access()

    def _verify_bucket_access(self):
        """Verify that we can access the S3 bucket"""
        try:
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            logger.info(f"Successfully connected to S3 bucket: {self.bucket_name}")
        except ClientError as e:
            error_code = e.response["Error"]["Code"]
            if error_code == "404":
                raise ValueError(f"S3 bucket '{self.bucket_name}' does not exist")
            elif error_code == "403":
                raise ValueError(f"Access denied to S3 bucket '{self.bucket_name}'")
            else:
                raise ValueError(f"Error accessing S3 bucket '{self.bucket_name}': {e}")
        except NoCredentialsError:
            raise ValueError(
                "AWS credentials not found. Please run 'aws configure' or set credentials in environment variables."
            )

    def _get_s3_key(self, filepath: str) -> str:
        """Get the full S3 key including prefix"""
        return f"{self.prefix}{filepath.lstrip('/')}"

    def save_file(
        self, content: str, filepath: str, metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """Save content to S3"""
        s3_key = self._get_s3_key(filepath)

        # Prepare metadata for S3
        s3_metadata = {
            "saved-at": datetime.now(timezone.utc).isoformat(),
            "content-type": "text/plain",
        }
        if metadata:
            # Convert metadata values to strings (S3 requirement)
            s3_metadata.update({k: str(v) for k, v in metadata.items()})

        try:
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=s3_key,
                Body=content.encode("utf-8"),
                ContentType="text/plain; charset=utf-8",
                Metadata=s3_metadata,
            )

            logger.info(
                f"Saved file to S3: {filepath}",
                bucket=self.bucket_name,
                s3_key=s3_key,
                size_bytes=len(content.encode("utf-8")),
            )

            return f"s3://{self.bucket_name}/{s3_key}"

        except ClientError as e:
            logger.error(f"Failed to save file to S3: {e}")
            raise

    def save_json(
        self,
        data: Dict[str, Any],
        filepath: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Save JSON data to S3"""
        s3_key = self._get_s3_key(filepath)

        save_data = data.copy()
        if metadata:
            save_data["metadata"] = {**save_data.get("metadata", {}), **metadata}

        json_content = json.dumps(save_data, indent=2, ensure_ascii=False)

        # Prepare metadata for S3
        s3_metadata = {
            "saved-at": datetime.now(timezone.utc).isoformat(),
            "content-type": "application/json",
        }
        if metadata:
            s3_metadata.update({k: str(v) for k, v in metadata.items()})

        try:
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=s3_key,
                Body=json_content.encode("utf-8"),
                ContentType="application/json; charset=utf-8",
                Metadata=s3_metadata,
            )

            logger.info(
                f"Saved JSON file to S3: {filepath}",
                bucket=self.bucket_name,
                s3_key=s3_key,
                size_bytes=len(json_content.encode("utf-8")),
            )

            return f"s3://{self.bucket_name}/{s3_key}"

        except ClientError as e:
            logger.error(f"Failed to save JSON file to S3: {e}")
            raise

    def file_exists(self, filepath: str) -> bool:
        """Check if file exists in S3"""
        s3_key = self._get_s3_key(filepath)

        try:
            self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)
            return True
        except ClientError as e:
            if e.response["Error"]["Code"] == "404":
                return False
            raise

    def get_file_url(self, filepath: str) -> str:
        """Get S3 URL for the file"""
        s3_key = self._get_s3_key(filepath)
        return f"s3://{self.bucket_name}/{s3_key}"


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/main.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# app/main.py
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from datetime import datetime

from .config import settings
from .api.v1 import api_router
from .utils.exceptions import ExtractionError
from .utils.logging_manager import LoggingManager
from .services.schema_registry import SchemaRegistry

# Configure logging
LoggingManager.configure_logging(level=settings.log_level, debug=settings.debug)

logger = LoggingManager.get_logger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan events"""
    # Startup
    logger.info("Starting FastAPI Data Extractor API")

    # Initialize schema registry
    SchemaRegistry.discover_schemas()
    logger.success("Schema registry initialized")

    yield

    # Shutdown
    logger.info("Shutting down FastAPI Data Extractor API")


def create_app() -> FastAPI:
    """Create and configure FastAPI application"""

    app = FastAPI(
        title=settings.app_name,
        version=settings.version,
        description="Advanced data extraction API with multiple input types and configurable storage",
        lifespan=lifespan,
        docs_url="/docs",
        redoc_url="/redoc",
    )

    # Configure CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # Configure appropriately for production
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Include API routes
    app.include_router(api_router, prefix="/api")

    # Global exception handlers
    @app.exception_handler(ExtractionError)
    async def extraction_error_handler(request, exc: ExtractionError):
        logger.error(
            f"Extraction error: {str(exc)}",
            extraction_type=getattr(exc, "extraction_type", None),
            details=getattr(exc, "details", {}),
        )
        return JSONResponse(
            status_code=422,
            content={
                "detail": str(exc),
                "type": "extraction_error",
                "extraction_type": getattr(exc, "extraction_type", None),
            },
        )

    @app.exception_handler(ValueError)
    async def value_error_handler(request, exc: ValueError):
        logger.error(f"Validation error: {str(exc)}")
        return JSONResponse(
            status_code=400, content={"detail": str(exc), "type": "validation_error"}
        )

    @app.exception_handler(Exception)
    async def general_exception_handler(request, exc: Exception):
        logger.error(f"Unexpected error: {str(exc)}", error_type=type(exc).__name__)
        return JSONResponse(
            status_code=500,
            content={"detail": "Internal server error", "type": "internal_error"},
        )

    # Root endpoint
    @app.get("/")
    async def read_root():
        """Root endpoint with API information"""
        return {
            "message": "FastAPI Data Extractor API",
            "version": settings.version,
            "docs_url": "/docs",
            "health_check": "/health",
            "storage_backend": settings.storage_backend,
        }

    # Health check endpoint (also available at root level)
    @app.get("/health")
    async def health_check():
        """Health check endpoint for Docker and monitoring"""
        try:
            # Test storage backend connection
            from .core.storage.factory import StorageFactory

            storage = StorageFactory.create_storage()
            storage_status = "healthy"
        except Exception as e:
            storage_status = f"error: {str(e)}"

        return {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "version": settings.version,
            "storage_backend": settings.storage_backend,
            "storage_status": storage_status,
            "environment": "development" if settings.debug else "production",
        }

    return app


# Create app instance
app = create_app()

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.debug,
        log_level=settings.log_level.lower(),
    )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/base.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Any, Dict, Optional, Type
from pydantic import BaseModel
from enum import Enum


class InputType(str, Enum):
    TEXT = "text"
    URL = "url"
    IMAGE = "image"
    YOUTUBE_URL = "youtube_url"
    AUDIO = "audio"


class BaseExtractionSchema(BaseModel):
    """Base class for all extraction schemas

    Subclasses should define these class attributes:
    - extraction_type: str - Unique identifier for this extraction type
    - description: str - Human-readable description of what this schema extracts
    - prompt: str - System prompt for OpenAI extraction
    """

    # These will be set by subclasses as class attributes
    # Don't define them here to avoid Pydantic field conflicts
    pass


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/requests.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Optional
from pydantic import BaseModel, Field, field_validator
from .base import InputType


class ExtractionRequest(BaseModel):
    input_type: InputType = Field(
        ..., description="Type of input: text, url, image, audio, or youtube_url"
    )
    content: str = Field(
        ...,
        description="Text content, URL, image path/base64, audio path/base64, or YouTube URL to process",
    )
    extraction_type: str = Field(..., description="Type of data to extract")

    # File management options
    save_markdown: bool = Field(
        default=False, description="Save scraped markdown content"
    )
    save_json: bool = Field(default=True, description="Save extracted JSON data")
    output_directory: Optional[str] = Field(
        default=None, description="Custom output directory"
    )
    filename_prefix: Optional[str] = Field(
        default=None, description="Custom filename prefix"
    )

    # Processing options
    custom_instructions: Optional[str] = Field(
        default=None, description="Additional extraction instructions"
    )

    @field_validator("content")
    @classmethod
    def validate_content(cls, v):
        if not v or not v.strip():
            raise ValueError("Content cannot be empty")
        return v.strip()

    # COMMENTA questo validator che causa il problema
    # @validator('extraction_type')
    # def validate_extraction_type(cls, v):
    #     from ..services.schema_registry import SchemaRegistry
    #     # Validate against available schemas at runtime
    #     available_types = SchemaRegistry.get_available_types()
    #     if v not in available_types:
    #         raise ValueError(f"Invalid extraction type: {v}. Available: {available_types}")
    #     return v


class AudioUploadRequest(BaseModel):
    """Request model for audio file uploads"""

    extraction_type: str = Field(
        ..., description="Type of data to extract from transcription"
    )

    # File management options
    save_markdown: bool = Field(
        default=False, description="Save transcription as markdown"
    )
    save_json: bool = Field(default=True, description="Save extracted JSON data")
    output_directory: Optional[str] = Field(
        default=None, description="Custom output directory"
    )
    filename_prefix: Optional[str] = Field(
        default=None, description="Custom filename prefix"
    )

    # Processing options
    custom_instructions: Optional[str] = Field(
        default=None, description="Additional extraction instructions"
    )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/responses.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field
from datetime import datetime

class ExtractionResult(BaseModel):
    success: bool = Field(..., description="Whether extraction was successful")
    extraction_type: str = Field(..., description="Type of extraction performed")
    extracted_data: Optional[Dict[str, Any]] = Field(default=None, description="Extracted structured data")
    
    # File paths
    markdown_path: Optional[str] = Field(default=None, description="Path to saved markdown file")
    json_path: Optional[str] = Field(default=None, description="Path to saved JSON file")
    
    # Metadata
    processing_time: float = Field(..., description="Processing time in seconds")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    token_usage: Optional[Dict[str, int]] = Field(default=None, description="OpenAI token usage")
    
    # Error handling
    error_message: Optional[str] = Field(default=None, description="Error message if extraction failed")
    warnings: List[str] = Field(default_factory=list, description="Processing warnings")


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/schemas/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# Import only recipe schema
from .recipes import RecipeExtraction

__all__ = ["RecipeExtraction"]


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/models/schemas/recipes.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
from typing import List, Optional, ClassVar
from pydantic import BaseModel, Field
from ..base import BaseExtractionSchema


class Ingredient(BaseModel):
    name: str = Field(..., description="Ingredient name")
    quantity: Optional[str] = Field(default=None, description="Ingredient quantity")
    unit: Optional[str] = Field(default=None, description="Unit of measurement")


class CookingStep(BaseModel):
    step_number: int = Field(..., description="Step order number")
    instruction: str = Field(..., description="Cooking instruction")
    duration: Optional[str] = Field(
        default=None, description="Time required for this step"
    )


class RecipeExtraction(BaseExtractionSchema):
    title: str = Field(..., description="Recipe title")
    description: Optional[str] = Field(default=None, description="Recipe description")
    prep_time: Optional[str] = Field(default=None, description="Preparation time")
    cook_time: Optional[str] = Field(default=None, description="Cooking time")
    servings: Optional[int] = Field(default=None, description="Number of servings")

    ingredients: List[Ingredient] = Field(..., description="List of ingredients")
    instructions: List[CookingStep] = Field(..., description="Cooking instructions")

    # Class attributes - use ClassVar to tell Pydantic these are NOT fields
    extraction_type: ClassVar[str] = "recipes"
    description: ClassVar[str] = (
        "Extracts recipe information including ingredients and instructions"
    )
    prompt: ClassVar[str] = (
        "Extract recipe information from the given content. Focus on ingredients with quantities and step-by-step instructions."
    )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/services/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/services/extraction_service.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import time
from typing import Optional

from ..core.content_scraper import ContentScraper
from ..core.extraction_engine import ExtractionEngine
from ..core.file_manager import FileManager
from ..models.base import InputType
from ..models.requests import ExtractionRequest
from ..models.responses import ExtractionResult
from ..utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)


class ExtractionService:
    """Service for extraction requests supporting multiple input types"""

    def __init__(self):
        self.scraper = ContentScraper()
        self.extraction_engine = ExtractionEngine()
        self.file_manager = FileManager()

    async def process_extraction(self, request: ExtractionRequest) -> ExtractionResult:
        """Process extraction request with multiple input types"""
        start_time = time.time()

        logger.info(
            f"Processing extraction request",
            extraction_type=request.extraction_type,
            input_type=request.input_type.value,
        )

        try:
            # For images, use direct extraction if it's recipes
            if (
                request.input_type == InputType.IMAGE
                and request.extraction_type == "recipes"
            ):
                extracted_data, token_usage, extraction_time = (
                    self.extraction_engine.extract_from_image_directly(
                        request.content,
                        request.extraction_type,
                        request.custom_instructions,
                    )
                )
                markdown_path = None  # No intermediate text for direct extraction
                content = "Direct image extraction - no intermediate text"
            else:
                # Standard flow: extract text first, then process
                if request.input_type == InputType.TEXT:
                    content = request.content
                    markdown_path = None
                    if request.save_markdown:
                        markdown_path = self.file_manager.save_markdown(
                            content,
                            None,
                            request.output_directory,
                            request.filename_prefix,
                        )
                else:
                    # Use ContentScraper for URL, YOUTUBE_URL, AUDIO (now async!)
                    content, markdown_path = await self.scraper.fetch_content(
                        request.content,
                        request.input_type.value,
                        request.save_markdown,
                        request.output_directory,
                    )

                # Extract structured data
                extracted_data, token_usage, extraction_time = (
                    self.extraction_engine.extract_structured_data(
                        content, request.extraction_type, request.custom_instructions
                    )
                )

            # Save JSON if requested
            json_path = None
            if request.save_json:
                json_path = self.file_manager.save_json(
                    extracted_data,
                    content,
                    request.extraction_type,
                    request.output_directory,
                    request.filename_prefix,
                )

            processing_time = time.time() - start_time

            logger.success(
                f"Successfully completed extraction",
                extraction_type=request.extraction_type,
                total_time=round(processing_time, 2),
                extraction_time=round(extraction_time, 2),
            )

            return ExtractionResult(
                success=True,
                extraction_type=request.extraction_type,
                extracted_data=extracted_data,
                markdown_path=markdown_path,
                json_path=json_path,
                processing_time=processing_time,
                token_usage=token_usage,
            )

        except Exception as e:
            processing_time = time.time() - start_time
            error_message = str(e)

            # Log the FULL error with traceback
            logger.error(
                f"Extraction failed: {error_message}",
                extraction_type=request.extraction_type,
                error=error_message,
                processing_time=round(processing_time, 2),
                exc_info=True,  # This will include the full traceback
            )

            return ExtractionResult(
                success=False,
                extraction_type=request.extraction_type,
                processing_time=processing_time,
                error_message=error_message,
            )


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/services/schema_registry.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
# app/services/schema_registry.py
import importlib
import pkgutil
from typing import Dict, Type, List, Any
from pathlib import Path

from ..models.base import BaseExtractionSchema
from ..utils.logging_manager import LoggingManager

logger = LoggingManager.get_logger(__name__)

class SchemaRegistry:
    """Auto-discovery registry for extraction schemas"""
    
    _schemas: Dict[str, Type[BaseExtractionSchema]] = {}
    _discovered = False
    
    @classmethod
    def discover_schemas(cls) -> None:
        """Automatically discover all schema classes"""
        if cls._discovered:
            return
            
        logger.info("Discovering extraction schemas...")
        
        # Get the schemas package path
        schemas_path = Path(__file__).parent.parent / "models" / "schemas"
        
        # Import all modules in the schemas package
        for module_info in pkgutil.iter_modules([str(schemas_path)]):
            if module_info.name.startswith('_'):
                continue
                
            try:
                module_name = f"app.models.schemas.{module_info.name}"
                module = importlib.import_module(module_name)
                
                # Find all BaseExtractionSchema subclasses in the module
                for attr_name in dir(module):
                    attr = getattr(module, attr_name)
                    
                    if (isinstance(attr, type) and 
                        issubclass(attr, BaseExtractionSchema) and 
                        attr != BaseExtractionSchema and
                        hasattr(attr, 'extraction_type') and
                        attr.extraction_type):
                        
                        extraction_type = attr.extraction_type
                        cls._schemas[extraction_type] = attr
                        logger.info(f"Discovered schema: {extraction_type} -> {attr.__name__}")
                        
            except Exception as e:
                logger.warning(f"Failed to load schema module {module_info.name}: {e}")
        
        cls._discovered = True
        logger.info(f"Schema discovery complete. Found {len(cls._schemas)} schemas.")
    
    @classmethod
    def get_schema(cls, extraction_type: str) -> Type[BaseExtractionSchema]:
        """Get schema class for extraction type"""
        cls.discover_schemas()
        
        if extraction_type not in cls._schemas:
            available = list(cls._schemas.keys())
            raise ValueError(f"Unknown extraction type: {extraction_type}. Available: {available}")
        
        return cls._schemas[extraction_type]
    
    @classmethod
    def get_all_schemas(cls) -> Dict[str, Dict[str, Any]]:
        """Get information about all available schemas"""
        cls.discover_schemas()
        
        result = {}
        for extraction_type, schema_class in cls._schemas.items():
            result[extraction_type] = {
                "name": extraction_type,
                "description": schema_class.description,
                "prompt": schema_class.prompt,
                "schema": schema_class.model_json_schema(),
                "class_name": schema_class.__name__
            }
        return result
    
    @classmethod
    def get_available_types(cls) -> List[str]:
        """Get list of available extraction types"""
        cls.discover_schemas()
        return list(cls._schemas.keys())
    
    @classmethod
    def reload_schemas(cls) -> None:
        """Force reload of all schemas (useful for development)"""
        cls._schemas.clear()
        cls._discovered = False
        cls.discover_schemas()

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/utils/__init__.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼


▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/utils/exceptions.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
"""Custom exceptions for the structured extraction API"""

class ExtractionError(Exception):
    """Base exception for extraction-related errors"""
    
    def __init__(self, message: str, extraction_type: str = None, details: dict = None):
        self.message = message
        self.extraction_type = extraction_type
        self.details = details or {}
        super().__init__(self.message)

class SchemaNotFoundError(ExtractionError):
    """Raised when requested extraction schema is not found"""
    
    def __init__(self, extraction_type: str, available_types: list = None):
        self.available_types = available_types or []
        message = f"Schema not found: {extraction_type}"
        if self.available_types:
            message += f". Available types: {', '.join(self.available_types)}"
        super().__init__(message, extraction_type)

class ContentScrapingError(ExtractionError):
    """Raised when content scraping fails"""
    
    def __init__(self, url: str, reason: str = None):
        self.url = url
        self.reason = reason
        message = f"Failed to scrape content from URL: {url}"
        if reason:
            message += f". Reason: {reason}"
        super().__init__(message, details={"url": url, "reason": reason})

class OpenAIExtractionError(ExtractionError):
    """Raised when OpenAI extraction fails"""
    
    def __init__(self, message: str, extraction_type: str = None, token_usage: dict = None):
        self.token_usage = token_usage or {}
        details = {"token_usage": self.token_usage}
        super().__init__(message, extraction_type, details)

class ValidationError(ExtractionError):
    """Raised when input validation fails"""
    
    def __init__(self, field: str, value: str, reason: str):
        self.field = field
        self.value = value
        self.reason = reason
        message = f"Validation error for field '{field}': {reason}"
        details = {"field": field, "value": value, "reason": reason}
        super().__init__(message, details=details)

class ConfigurationError(ExtractionError):
    """Raised when there's a configuration issue"""
    
    def __init__(self, setting: str, message: str):
        self.setting = setting
        full_message = f"Configuration error for '{setting}': {message}"
        details = {"setting": setting}
        super().__init__(full_message, details=details)

▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
FILE: app/utils/logging_manager.py
▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼
import sys
from typing import Optional
from loguru import logger


class LoggingManager:
    """
    Centralized logging manager using Loguru
    Provides structured logging with consistent formatting
    """
    
    _configured = False
    
    @classmethod
    def configure_logging(cls, level: str = "INFO", debug: bool = False) -> None:
        """Configure loguru logger with appropriate settings"""
        if cls._configured:
            return
            
        # Remove default handler
        logger.remove()
        
        # Configure log level based on debug mode
        log_level = "DEBUG" if debug else level
        
        # Add console handler with structured format
        logger.add(
            sys.stdout,
            level=log_level,
            format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
                   "<level>{level: <8}</level> | "
                   "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
                   "<level>{message}</level>",
            colorize=True,
            diagnose=debug
        )
        
        # Add file handler for errors
        logger.add(
            "logs/error.log",
            level="ERROR",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}",
            rotation="10 MB",
            retention="30 days",
            compression="zip"
        )
        
        # Add file handler for all logs if debug
        if debug:
            logger.add(
                "logs/debug.log",
                level="DEBUG",
                format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}",
                rotation="50 MB",
                retention="7 days",
                compression="zip"
            )
        
        cls._configured = True
        logger.info("Logging configured successfully")
    
    @classmethod
    def get_logger(cls, name: Optional[str] = None):
        """Get a logger instance with optional name binding"""
        if not cls._configured:
            cls.configure_logging()
        
        if name:
            return logger.bind(name=name)
        return logger
    
    @classmethod
    def add_file_handler(cls, file_path: str, level: str = "INFO", 
                        rotation: str = "10 MB", retention: str = "30 days") -> None:
        """Add additional file handler"""
        logger.add(
            file_path,
            level=level,
            format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}",
            rotation=rotation,
            retention=retention,
            compression="zip"
        )
        logger.info(f"Added file handler: {file_path}")
    
    @classmethod
    def set_level(cls, level: str) -> None:
        """Change logging level dynamically"""
        # This would require more complex implementation to change existing handlers
        logger.info(f"Logging level change requested: {level}") 

================================================================================
PROCESSING SUMMARY
Files included: 32
Files/directories excluded: 24
Repository analyzed: fastapi-data-extractor
================================================================================
